<div itemscope itemtype="http://developers.google.com/ReferenceObject">
<meta itemprop="name" content="tfa.activations" />
<meta itemprop="path" content="Stable" />
</div>

# Module: tfa.activations


<table class="tfo-notebook-buttons tfo-api" align="left">

<td>
  <a target="_blank" href="https://github.com/tensorflow/addons/tree/r0.7/tensorflow_addons/activations/__init__.py">
    <img src="https://www.tensorflow.org/images/GitHub-Mark-32px.png" />
    View source on GitHub
  </a>
</td></table>



Addititonal activation functions.



## Functions

[`gelu(...)`](../tfa/activations/gelu.md): Gaussian Error Linear Unit.

[`hardshrink(...)`](../tfa/activations/hardshrink.md): Hard shrink function.

[`lisht(...)`](../tfa/activations/lisht.md): LiSHT: Non-Parameteric Linearly Scaled Hyperbolic Tangent Activation Function.

[`mish(...)`](../tfa/activations/mish.md): Mish: A Self Regularized Non-Monotonic Neural Activation Function.

[`rrelu(...)`](../tfa/activations/rrelu.md): rrelu function.

[`softshrink(...)`](../tfa/activations/softshrink.md): Soft shrink function.

[`sparsemax(...)`](../tfa/activations/sparsemax.md): Sparsemax activation function [1].

[`tanhshrink(...)`](../tfa/activations/tanhshrink.md): Applies the element-wise function: x - tanh(x)



