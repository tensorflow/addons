{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2021 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "# TensorFlow Addons Layers: CRF\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/addons/tutorials/layers_crf\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/addons/blob/master/docs/tutorials/layers_crf.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/addons/blob/master/docs/tutorials/layers_crf.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "      <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/addons/docs/tutorials/layers_crf.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHxb-dlhMIzW"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook will demonstrate how to use the CRF (Conditional Random Field) layer in TensorFlow Addons.\n",
        "We will introduce how to use the CRF layer by building a NER extraction model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUXex9ctTuDB"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rEk-ibQkDNtF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.1 MB 8.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 264 kB 8.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 50 kB 9.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 119 kB 72.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 243 kB 60.3 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -U -q tensorflow-addons\n",
        "!pip install -q tensorflow\n",
        "!pip install -q datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "IqR2PQG4ZaZ0"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cb5bb13a189"
      },
      "source": [
        "## Traning data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3db2a84b25a7"
      },
      "source": [
        "We load the CoNLL 2003 dataset by using the datasets library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5WctVm30TFsT"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9c5d7857ac3a42bab4254a83a1b6b792",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/2.60k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d7783c9a438542cd86ecc84dbb6b58e1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.78k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading and preparing dataset conll2003/conll2003 (download: 4.63 MiB, generated: 9.78 MiB, post-processed: Unknown size, total: 14.41 MiB) to /root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "332e22e368294251a8ae05be0a604728",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/650k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f39e1bfdc1304cf48a1c6a2c1e9d5662",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/163k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c2b63d7ec60743eab50fe12e4d48119e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/146k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "30636a81754b468a9abb99e599635976",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a797b5e1f93b491ca8c5d3e7e8aa7f41",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c2c567ea31a746359ff491f1eed8384d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset conll2003 downloaded and prepared to /root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6. Subsequent calls will reuse this data.\n"
          ]
        }
      ],
      "source": [
        "conll_data = datasets.load_dataset(\"conll2003\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUfW7yZyunU4"
      },
      "source": [
        "Inspect the data splits and features:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Xh-sQGxGy-Tf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
              "        num_rows: 14041\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
              "        num_rows: 3250\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
              "        num_rows: 3453\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conll_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zsd8RqnjuvFh"
      },
      "source": [
        "Get a sample of train data and print it out:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8QLFEa2-TxhQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']\n",
            "[3, 0, 7, 0, 0, 0, 7, 0, 0]\n"
          ]
        }
      ],
      "source": [
        "for item in conll_data[\"train\"]:\n",
        "  sample_tokens = item['tokens']\n",
        "  sample_tag_ids = item[\"ner_tags\"]\n",
        "  print(sample_tokens)\n",
        "  print(sample_tag_ids)\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkCmPUPjvBC-"
      },
      "source": [
        "For our NER model, the input are the tokens which is a list of strings. The outputs are the NER tags which in the dataset they are the tag ids."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiRQdcVNvngI"
      },
      "source": [
        "The dataset also give the information about the mapping of NER tags and ids."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "RugRp-kywNge"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n"
          ]
        }
      ],
      "source": [
        "dataset_builder = datasets.load_dataset_builder('conll2003')\n",
        "raw_tags = dataset_builder.info.features['ner_tags'].feature.names\n",
        "print(raw_tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eQvbO_Lv1KP"
      },
      "source": [
        "Let us decode the NER tag ids to tags."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Z_zH45qZwsSv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']\n",
            "['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']\n"
          ]
        }
      ],
      "source": [
        "sample_tags = [raw_tags[i] for i in sample_tag_ids]\n",
        "\n",
        "print(sample_tokens)\n",
        "print(sample_tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlXX1Qegv8-f"
      },
      "source": [
        "Those tags are used to encode the named entities by some format. In this dataset, tags are encoded in [IOB](https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)) format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBR_sqqAw4fA"
      },
      "source": [
        "We add a special tag `<PAD>` to the tag set which is used to represent a padding in the sequence. In NLP, we usually use 0 to mark padding. This is the default setting for many functions in Machine Learning software (include TensorFlow)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-67uGdIOzWDf"
      },
      "source": [
        "We create a list so we can get the tags from ids."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-NChlWMnUTol"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['<PAD>', 'O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n"
          ]
        }
      ],
      "source": [
        "tags = ['<PAD>'] + raw_tags\n",
        "print(tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d45nlEDXWqG6"
      },
      "source": [
        "Define some constant we will used for later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "9A2UNwDnaPkX"
      },
      "outputs": [],
      "source": [
        "TAG_SIZE = len(tags)\n",
        "VOCAB_SIZE = 20000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8XXrsg1Wvvb"
      },
      "source": [
        "Building vocabulary lookup layer for tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "l3WJsqUBxpFX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20000\n",
            "['[MASK]', '[UNK]', 'the', '.', ',', 'of', 'in', 'to', 'a', 'and']\n"
          ]
        }
      ],
      "source": [
        "train_tokens = tf.ragged.constant(conll_data[\"train\"][\"tokens\"])\n",
        "train_tokens = tf.map_fn(tf.strings.lower, train_tokens)\n",
        "\n",
        "lookup_layer = tf.keras.layers.StringLookup(max_tokens=VOCAB_SIZE, mask_token=\"[MASK]\", oov_token=\"[UNK]\")\n",
        "lookup_layer.adapt(train_tokens)\n",
        "\n",
        "print(len(lookup_layer.get_vocabulary()))\n",
        "print(lookup_layer.get_vocabulary()[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lanszT8uXA6y"
      },
      "source": [
        "Creating raw (without preprocess) train and validation dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "j-u70kbtTSGg"
      },
      "outputs": [],
      "source": [
        "def create_data_generator(dataset):\n",
        "  def data_generator():\n",
        "    for item in dataset:\n",
        "      yield item['tokens'], item['ner_tags']\n",
        "  \n",
        "  return data_generator\n",
        "\n",
        "data_signature= (\n",
        "        tf.TensorSpec(shape=(None,), dtype=tf.string),\n",
        "        tf.TensorSpec(shape=(None, ), dtype=tf.int32)\n",
        ")\n",
        "\n",
        "train_data = tf.data.Dataset.from_generator(\n",
        "    create_data_generator(conll_data[\"train\"]),\n",
        "    output_signature=data_signature\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWuGTDeLXQ2D"
      },
      "source": [
        "Creating train and validation dataset that can be used for traning and validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "EArTl8EBbCUF"
      },
      "outputs": [],
      "source": [
        "def dataset_preprocess(tokens, tag_ids):\n",
        "    preprocessed_tokens = preprecess_tokens(tokens)\n",
        "\n",
        "    # increase by 1 for all tag_ids,\n",
        "    # because we add `<PAD>` as the first element in tags list\n",
        "    preprocessed_tag_ids = tag_ids + 1\n",
        "\n",
        "    return preprocessed_tokens, preprocessed_tag_ids\n",
        "\n",
        "def preprecess_tokens(tokens):\n",
        "    tokens = tf.strings.lower(tokens)\n",
        "    return lookup_layer(tokens)\n",
        "\n",
        "BATCH_SIZE = 2048\n",
        "\n",
        "# With `padded_batch()`, each batch may have different length\n",
        "# shape: (batch_size, None)\n",
        "train_dataset = (\n",
        "    train_data.map(dataset_preprocess)\n",
        "    .padded_batch(batch_size=BATCH_SIZE).cache()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPjyvdqaMEhU"
      },
      "source": [
        "## Method one: Using the CRF layer in a custom training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f90960e1c871"
      },
      "source": [
        "### Creating model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dcba022b4d6"
      },
      "source": [
        "Define BiLSTM+CRF model by using tfa.layers.CRF layer.\n",
        "The CRF layer not only ouput the CRF decode result (`decode_sequence`), but also outupt some interal variables (`potentials`, `sequence_length` and `kernel`). You will use those internal variables for compute loss value later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "KtylpxOmceaC"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py:376: UserWarning: CRF decoding models have serialization issues in TF >=2.5 . Please see isse #2476\n",
            "  return py_builtins.overload_of(f)(*args)\n"
          ]
        }
      ],
      "source": [
        "# Build the model\n",
        "def build_embedding_bilstm_crf_model(\n",
        "    vocab_size: int, embed_dims: int, lstm_unit: int, tag_size: int\n",
        ") -> tf.keras.Model:\n",
        "    x = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name=\"x\")\n",
        "    y = tf.keras.layers.Embedding(vocab_size, embed_dims, mask_zero=True)(x)\n",
        "    y = tf.keras.layers.Bidirectional(\n",
        "        tf.keras.layers.LSTM(lstm_unit, return_sequences=True)\n",
        "    )(y)\n",
        "    decode_sequence, potentials, sequence_length, kernel = tfa.layers.CRF(tag_size)(y)\n",
        "\n",
        "    return tf.keras.Model(\n",
        "        inputs=x, outputs=[decode_sequence, potentials, sequence_length, kernel]\n",
        "    )\n",
        "\n",
        "\n",
        "model = build_embedding_bilstm_crf_model(VOCAB_SIZE, 32, 64, TAG_SIZE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwdM2pl3RSPb"
      },
      "source": [
        "Run the model on a single batch of data, and inspect the output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "WVMHNNbUXprr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor([3 6 2 3 6 2 3 6 2], shape=(9,), dtype=int32)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow_addons/text/crf.py:546: UserWarning: CRF decoding models have serialization issues in TF >=2.5 . Please see isse #2476\n",
            "  \"CRF decoding models have serialization issues in TF >=2.5 . Please see isse #2476\"\n"
          ]
        }
      ],
      "source": [
        "# preprocess\n",
        "preprecessd_tokens = preprecess_tokens(sample_tokens)\n",
        "\n",
        "# expand the tensor to shape: [1, None]. That is add batch dim\n",
        "inputs = tf.expand_dims(preprecessd_tokens, axis=0)\n",
        "\n",
        "outputs, *_ = model(inputs)\n",
        "print(outputs[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uabQmjMtRtzs"
      },
      "source": [
        "### Define CRF loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f54910109035"
      },
      "source": [
        "By using the real y and some internal variables of the CRF layer. You can compute the log likelihood of real y. Use the negative of log likelihood as the loss to optimize."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "3b38225d9464"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def crf_loss_func(potentials, sequence_length, kernel, y):\n",
        "    crf_likelihood, _ = tfa.text.crf_log_likelihood(\n",
        "        potentials, y, sequence_length, kernel\n",
        "    )\n",
        "    # likelihood to loss\n",
        "    flat_crf_loss = -1 * crf_likelihood\n",
        "    crf_loss = tf.reduce_mean(flat_crf_loss)\n",
        "\n",
        "    return crf_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fb535bfc09d"
      },
      "source": [
        "### Define optimizer, metrics and train_step fucntion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "U82B_tH2d294"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam(0.02)\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "\n",
        "@tf.function(experimental_relax_shapes=True)\n",
        "def train_step(x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        decoded_sequence, potentials, sequence_length, kernel = model(x)\n",
        "        crf_loss = crf_loss_func(potentials, sequence_length, kernel, y)\n",
        "        loss = crf_loss + tf.reduce_sum(model.losses)\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    train_loss(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70ecdcdf800d"
      },
      "source": [
        "### Training model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "04b19e455b07"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py:376: UserWarning: CRF decoding models have serialization issues in TF >=2.5 . Please see isse #2476\n",
            "  return py_builtins.overload_of(f)(*args)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 17.154226303100586\n",
            "Epoch 2, Loss: 8.932731628417969\n",
            "Epoch 3, Loss: 5.642122745513916\n",
            "Epoch 4, Loss: 4.217898845672607\n",
            "Epoch 5, Loss: 3.090301275253296\n",
            "Epoch 6, Loss: 1.8915987014770508\n",
            "Epoch 7, Loss: 1.1709725856781006\n",
            "Epoch 8, Loss: 0.81854248046875\n",
            "Epoch 9, Loss: 0.6320579648017883\n",
            "Epoch 10, Loss: 0.5205766558647156\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    # Reset the metrics at the start of the next epoch\n",
        "    train_loss.reset_states()\n",
        "\n",
        "    for x, y in train_dataset:\n",
        "        train_step(x, y)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, \" f\"Loss: {train_loss.result()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c80d7afb8d0e"
      },
      "source": [
        "### Making inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8cc7e6acacf"
      },
      "source": [
        "Inspect the predict result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "F4xiJs1mi2yN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "raw inputs:  ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py:376: UserWarning: CRF decoding models have serialization issues in TF >=2.5 . Please see isse #2476\n",
            "  return py_builtins.overload_of(f)(*args)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ground true tags:  ['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']\n",
            "predicted tags:  ['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']\n"
          ]
        }
      ],
      "source": [
        "# print the inputs and expected outputs\n",
        "print(\"raw inputs: \", sample_tokens)\n",
        "\n",
        "# preprocess\n",
        "preprocessed_inputs = preprecess_tokens(\n",
        "    sample_tokens\n",
        ")\n",
        "# expend the batch dim\n",
        "inputs = tf.reshape(preprocessed_inputs, shape=[1, -1])\n",
        "\n",
        "outputs, *_ = model.predict(inputs)\n",
        "prediction = [tags[i] for i in outputs[0]]\n",
        "\n",
        "# Keypoint: EU -> B-ORG, German -> B-MISC, British -> B-MISC\n",
        "print(\"ground true tags: \", sample_tags)\n",
        "print(\"predicted tags: \", prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_q1h8vQhMvKi"
      },
      "source": [
        "## Method two: Using the CRF layer via model subclassing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVuODB-cN9le"
      },
      "source": [
        "### Creating the base model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBqjfSFiN9lf"
      },
      "source": [
        "Define the BiLSTM model as the base model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "_oMkjdoIN9lf"
      },
      "outputs": [],
      "source": [
        "# Build the model\n",
        "def build_embedding_bilstm_crf_model(\n",
        "    vocab_size: int, embed_dims: int, lstm_unit: int\n",
        ") -> tf.keras.Model:\n",
        "    x = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name=\"x\")\n",
        "    y = tf.keras.layers.Embedding(vocab_size, embed_dims, mask_zero=True)(x)\n",
        "    y = tf.keras.layers.Bidirectional(\n",
        "        tf.keras.layers.LSTM(lstm_unit, return_sequences=True)\n",
        "    )(y)\n",
        "\n",
        "    return tf.keras.Model(\n",
        "        inputs=x, outputs=y\n",
        "    )\n",
        "\n",
        "\n",
        "base_model = build_embedding_bilstm_crf_model(VOCAB_SIZE, 32, 64)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Gwcy02YN9lf"
      },
      "source": [
        "Run the model on a single batch of data, and inspect the output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "h-kBqrTxN9lf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[-0.00246301  0.00139769  0.00138668 ... -0.0066145  -0.00208775\n",
            "  -0.00464685]\n",
            " [-0.00359022 -0.00279237  0.00176504 ... -0.00154561  0.00373552\n",
            "  -0.00553161]\n",
            " [-0.00238984 -0.00648718  0.00698042 ...  0.00294961  0.00392122\n",
            "  -0.00589067]\n",
            " ...\n",
            " [-0.0012426   0.00084688  0.00371827 ...  0.00063269  0.00224524\n",
            "   0.00329087]\n",
            " [ 0.00435535  0.00248722  0.00835475 ...  0.00167271  0.00393107\n",
            "   0.00167722]\n",
            " [ 0.00835333  0.0062149   0.00396942 ...  0.0044266   0.00098306\n",
            "   0.00133276]], shape=(9, 128), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "# preprocess\n",
        "preprecessd_tokens = preprecess_tokens(sample_tokens)\n",
        "\n",
        "# expand the tensor to shape: [1, None]. That is add batch dim\n",
        "inputs = tf.expand_dims(preprecessd_tokens, axis=0)\n",
        "\n",
        "outputs = base_model(inputs)\n",
        "print(outputs[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jg_qCpkOVLk"
      },
      "source": [
        "### Creating CRF model wrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "oySs_9WQNznV"
      },
      "outputs": [],
      "source": [
        "class CRFModelWrapper(tf.keras.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: tf.keras.Model,\n",
        "        units: int,\n",
        "        chain_initializer=\"orthogonal\",\n",
        "        use_boundary: bool = True,\n",
        "        boundary_initializer=\"zeros\",\n",
        "        use_kernel: bool = True,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.crf_layer = tfa.layers.CRF(\n",
        "            units=units,\n",
        "            chain_initializer=chain_initializer,\n",
        "            use_boundary=use_boundary,\n",
        "            boundary_initializer=boundary_initializer,\n",
        "            use_kernel=use_kernel,\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "        self.base_model = model\n",
        "\n",
        "    def unpack_training_data(self, data):\n",
        "        # override me, if this is not suit for your task\n",
        "        if len(data) == 3:\n",
        "            x, y, sample_weight = data\n",
        "        else:\n",
        "            x, y = data\n",
        "            sample_weight = None\n",
        "        return x, y, sample_weight\n",
        "\n",
        "    def call(self, inputs, training=None, mask=None, return_crf_internal=False):\n",
        "        base_model_outputs = self.base_model(inputs, training, mask)\n",
        "\n",
        "        # change next line, if your model has more outputs\n",
        "        crf_input = base_model_outputs\n",
        "\n",
        "        decode_sequence, potentials, sequence_length, kernel = self.crf_layer(crf_input)\n",
        "\n",
        "        # change next line, if your base model has more outputs\n",
        "        # Aways keep `(potentials, sequence_length, kernel), decode_sequence, `\n",
        "        # as first two outputs of model.\n",
        "        # current `self.train_step()` expected such settings\n",
        "        outputs = (potentials, sequence_length, kernel), decode_sequence\n",
        "\n",
        "        if return_crf_internal:\n",
        "            return outputs\n",
        "        else:\n",
        "            # outputs[0] is the crf internal, skip it\n",
        "            output_without_crf_internal = outputs[1:]\n",
        "\n",
        "            # it is nicer to return a tensor instead of an one tensor list\n",
        "            if len(output_without_crf_internal) == 1:\n",
        "                return output_without_crf_internal[0]\n",
        "            else:\n",
        "                return output_without_crf_internal\n",
        "\n",
        "    def compute_crf_loss(self, potentials, sequence_length, kernel, y, sample_weight=None):\n",
        "        crf_likelihood, _ = tfa.text.crf_log_likelihood(\n",
        "            potentials, y, sequence_length, kernel\n",
        "        )\n",
        "        # convert likelihood to loss\n",
        "        flat_crf_loss = -1 * crf_likelihood\n",
        "        if sample_weight is not None:\n",
        "            flat_crf_loss = flat_crf_loss * sample_weight\n",
        "        crf_loss = tf.reduce_mean(flat_crf_loss)\n",
        "\n",
        "        return crf_loss\n",
        "\n",
        "    def train_step(self, data):\n",
        "        x, y, sample_weight = self.unpack_training_data(data)\n",
        "        with tf.GradientTape() as tape:\n",
        "            (potentials, sequence_length, kernel), decoded_sequence, *_ = self(\n",
        "                x, training=True, return_crf_internal=True\n",
        "            )\n",
        "            crf_loss = self.compute_crf_loss(\n",
        "                potentials, sequence_length, kernel, y, sample_weight\n",
        "            )\n",
        "            loss = crf_loss + tf.reduce_sum(self.losses)\n",
        "        gradients = tape.gradient(loss, self.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "\n",
        "        # Update metrics (includes the metric that tracks the loss)\n",
        "        self.compiled_metrics.update_state(y, decoded_sequence)\n",
        "        # Return a dict mapping metric names to current value\n",
        "        results = {m.name: m.result() for m in self.metrics}\n",
        "        results.update({\"loss\": loss, \"crf_loss\": crf_loss})  # append loss\n",
        "        return results\n",
        "\n",
        "    def test_step(self, data):\n",
        "        x, y, sample_weight = self.unpack_training_data(data)\n",
        "        (potentials, sequence_length, kernel), decode_sequence, *_ = self(\n",
        "            x, training=False, return_crf_internal=True\n",
        "        )\n",
        "        crf_loss = self.compute_crf_loss(\n",
        "            potentials, sequence_length, kernel, y, sample_weight\n",
        "        )\n",
        "        loss = crf_loss + tf.reduce_sum(self.losses)\n",
        "        # Update metrics (includes the metric that tracks the loss)\n",
        "        self.compiled_metrics.update_state(y, decode_sequence)\n",
        "        # Return a dict mapping metric names to current value\n",
        "        results = {m.name: m.result() for m in self.metrics}\n",
        "        results.update({\"loss\": loss, \"crf_loss\": crf_loss})  # append loss\n",
        "        return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRY6ZrmmOjGC"
      },
      "source": [
        "### Wrapper base model with CRF model wrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "joWipZyfOrix"
      },
      "outputs": [],
      "source": [
        "model = CRFModelWrapper(base_model, TAG_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSiBpyeVO_hg"
      },
      "source": [
        "### Traning model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "pFMeRQTrPI1B"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py:376: UserWarning: CRF decoding models have serialization issues in TF >=2.5 . Please see isse #2476\n",
            "  return py_builtins.overload_of(f)(*args)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7/7 - 9s - loss: 10.3882 - crf_loss: 10.3882\n",
            "Epoch 2/10\n",
            "7/7 - 1s - loss: 6.8143 - crf_loss: 6.8143\n",
            "Epoch 3/10\n",
            "7/7 - 1s - loss: 4.8042 - crf_loss: 4.8042\n",
            "Epoch 4/10\n",
            "7/7 - 1s - loss: 3.5163 - crf_loss: 3.5163\n",
            "Epoch 5/10\n",
            "7/7 - 1s - loss: 2.0655 - crf_loss: 2.0655\n",
            "Epoch 6/10\n",
            "7/7 - 1s - loss: 1.3112 - crf_loss: 1.3112\n",
            "Epoch 7/10\n",
            "7/7 - 1s - loss: 0.9268 - crf_loss: 0.9268\n",
            "Epoch 8/10\n",
            "7/7 - 1s - loss: 0.6875 - crf_loss: 0.6875\n",
            "Epoch 9/10\n",
            "7/7 - 1s - loss: 0.5071 - crf_loss: 0.5071\n",
            "Epoch 10/10\n",
            "7/7 - 1s - loss: 0.3823 - crf_loss: 0.3823\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f69392ffdd0>"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(0.02))\n",
        "\n",
        "model.fit(train_dataset, epochs=10, verbose=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2s9PmHePzVr"
      },
      "source": [
        "### Making inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhsUyaOGPzVx"
      },
      "source": [
        "Inspect the predict result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "MpTr22-XPzVx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "raw inputs:  ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py:376: UserWarning: CRF decoding models have serialization issues in TF >=2.5 . Please see isse #2476\n",
            "  return py_builtins.overload_of(f)(*args)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ground true tags:  ['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']\n",
            "predicted tags:  ['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']\n"
          ]
        }
      ],
      "source": [
        "# print the inputs and expected outputs\n",
        "print(\"raw inputs: \", sample_tokens)\n",
        "\n",
        "# preprocess\n",
        "preprocessed_inputs = preprecess_tokens(\n",
        "    sample_tokens\n",
        ")\n",
        "# expend the batch dim\n",
        "inputs = tf.reshape(preprocessed_inputs, shape=[1, -1])\n",
        "\n",
        "outputs = model.predict(inputs)\n",
        "prediction = [tags[i] for i in outputs[0]]\n",
        "\n",
        "# Keypoint: EU -> B-ORG, German -> B-MISC, British -> B-MISC\n",
        "print(\"ground true tags: \", sample_tags)\n",
        "print(\"predicted tags: \", prediction)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Tce3stUlHN0L"
      ],
      "name": "layers_crf.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
