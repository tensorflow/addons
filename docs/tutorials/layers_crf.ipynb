{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2021 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "# TensorFlow Addons Layers: CRF\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/addons/tutorials/layers_crf\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/addons/blob/master/docs/tutorials/layers_crf.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/addons/blob/master/docs/tutorials/layers_crf.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "      <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/addons/docs/tutorials/layers_crf.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHxb-dlhMIzW"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook will demonstrate implementing the Conditional Random Field (CRF) in TensorFlow. TensorFlow Addons provides CRF (more precisely, linear chain CRF) layer to users. In Natural Language Processing (NLP), linear chain CRFs are popular, for which each prediction can make use of information from its neighbors. Therefore, it can improve the performance of the base model. For more information about CRF, please visit [Wikipedia](https://en.wikipedia.org/wiki/Conditional_random_field).\n",
        "\n",
        "In the following sections, you will learn how to use the CRF layer in two ways by building Named Entity Recognition (NER) models. NER is the task of tagging entities (e.g., \"Winston Churchill\", \"London\", \"tomorrow\") in text with their corresponding type (e.g., \"Person\", \"City\", \"Date\"). NER is an important and popular task in NLP."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUXex9ctTuDB"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70a92b56113b"
      },
      "source": [
        "In addition to TensorFlow and TensorFlow addons, you will also need to install `datasets`. The package `datasets` (homepage: https://github.com/huggingface/datasets) comes from the HuggingFace team. It is a community-driven open-source library of datasets. A dataset from this package will be used in later projects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEk-ibQkDNtF"
      },
      "outputs": [],
      "source": [
        "!pip install -q tensorflow-addons  # version >= 0.15.0 is required\n",
        "!pip install -q tensorflow\n",
        "!pip install -q datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d424f5444ac7"
      },
      "source": [
        "Import modules so you can use them later:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqR2PQG4ZaZ0"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cb5bb13a189"
      },
      "source": [
        "## Training data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3db2a84b25a7"
      },
      "source": [
        "In this tutorial, CoNLL 2003 dataset will be used for training. The CoNLL 2003 is a language-independent named entity recognition dataset released as a part of CoNLL 2003 shared task. For more information, please visit https://huggingface.co/datasets/conll2003.\n",
        "\n",
        "With the help of the `datasets` package, the data loading is quite easy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WctVm30TFsT"
      },
      "outputs": [],
      "source": [
        "conll_data = datasets.load_dataset(\"conll2003\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUfW7yZyunU4"
      },
      "source": [
        "Inspect the data splits and features:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xh-sQGxGy-Tf"
      },
      "outputs": [],
      "source": [
        "conll_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zsd8RqnjuvFh"
      },
      "source": [
        "Get a sample of train data and print it out:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8QLFEa2-TxhQ"
      },
      "outputs": [],
      "source": [
        "for item in conll_data[\"train\"]:\n",
        "  sample_tokens = item['tokens']\n",
        "  sample_tag_ids = item[\"ner_tags\"]\n",
        "  print(sample_tokens)\n",
        "  print(sample_tag_ids)\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkCmPUPjvBC-"
      },
      "source": [
        "For our NER model, the inputs are the tokens which is a list of strings. The outputs are the NER tags which in the dataset they are the tag ids."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiRQdcVNvngI"
      },
      "source": [
        "The dataset also give the information about the mapping of NER tags and ids:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RugRp-kywNge"
      },
      "outputs": [],
      "source": [
        "dataset_builder = datasets.load_dataset_builder('conll2003')\n",
        "raw_tags = dataset_builder.info.features['ner_tags'].feature.names\n",
        "print(raw_tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eQvbO_Lv1KP"
      },
      "source": [
        "Let's decode the NER tag ids to tags:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_zH45qZwsSv"
      },
      "outputs": [],
      "source": [
        "sample_tags = [raw_tags[i] for i in sample_tag_ids]\n",
        "\n",
        "print(list(zip(sample_tokens, sample_tags)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlXX1Qegv8-f"
      },
      "source": [
        "Those tags are used to encode the named entities by some format. In this dataset, tags are encoded in [IOB](https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)) format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBR_sqqAw4fA"
      },
      "source": [
        "Add a special tag `<PAD>` to the tag set, which is used to represent padding in the sequence. In NLP, 0 is usually used to mark padding. This is the default setting for many functions in Machine Learning software (including TensorFlow)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-67uGdIOzWDf"
      },
      "source": [
        "Create a list to convert tag ids to tag text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NChlWMnUTol"
      },
      "outputs": [],
      "source": [
        "tags = ['<PAD>'] + raw_tags\n",
        "print(tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d45nlEDXWqG6"
      },
      "source": [
        "Define some constants which will be used later:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9A2UNwDnaPkX"
      },
      "outputs": [],
      "source": [
        "TAG_SIZE = len(tags)\n",
        "VOCAB_SIZE = 20000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8XXrsg1Wvvb"
      },
      "source": [
        "Building vocabulary lookup layer for tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3WJsqUBxpFX"
      },
      "outputs": [],
      "source": [
        "train_tokens = tf.ragged.constant(conll_data[\"train\"][\"tokens\"])\n",
        "train_tokens = tf.map_fn(tf.strings.lower, train_tokens)\n",
        "\n",
        "lookup_layer = tf.keras.layers.StringLookup(max_tokens=VOCAB_SIZE, mask_token=\"[MASK]\", oov_token=\"[UNK]\")\n",
        "lookup_layer.adapt(train_tokens)\n",
        "\n",
        "print(len(lookup_layer.get_vocabulary()))\n",
        "print(lookup_layer.get_vocabulary()[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lanszT8uXA6y"
      },
      "source": [
        "Let's create a raw (without preprocess) train dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-u70kbtTSGg"
      },
      "outputs": [],
      "source": [
        "def create_data_generator(dataset):\n",
        "  def data_generator():\n",
        "    for item in dataset:\n",
        "      yield item['tokens'], item['ner_tags']\n",
        "  \n",
        "  return data_generator\n",
        "\n",
        "data_signature= (\n",
        "        tf.TensorSpec(shape=(None,), dtype=tf.string),\n",
        "        tf.TensorSpec(shape=(None, ), dtype=tf.int32)\n",
        ")\n",
        "\n",
        "train_data = tf.data.Dataset.from_generator(\n",
        "    create_data_generator(conll_data[\"train\"]),\n",
        "    output_signature=data_signature\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWuGTDeLXQ2D"
      },
      "source": [
        "Let's preprocess the training dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EArTl8EBbCUF"
      },
      "outputs": [],
      "source": [
        "def dataset_preprocess(tokens, tag_ids):\n",
        "    preprocessed_tokens = preprecess_tokens(tokens)\n",
        "\n",
        "    # increase by 1 for all tag_ids,\n",
        "    # because `<PAD>` is added as the first element in tags list\n",
        "    preprocessed_tag_ids = tag_ids + 1\n",
        "\n",
        "    return preprocessed_tokens, preprocessed_tag_ids\n",
        "\n",
        "def preprecess_tokens(tokens):\n",
        "    tokens = tf.strings.lower(tokens)\n",
        "    return lookup_layer(tokens)\n",
        "\n",
        "BATCH_SIZE = 2048\n",
        "\n",
        "# With `padded_batch()`, each batch may have different length\n",
        "# shape: (batch_size, None)\n",
        "train_dataset = (\n",
        "    train_data.map(dataset_preprocess)\n",
        "    .padded_batch(batch_size=BATCH_SIZE).cache()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPjyvdqaMEhU"
      },
      "source": [
        "## Method one: Using the CRF layer in a custom training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4602595e50db"
      },
      "source": [
        "Using a custom training loop is a powerful way to customize the model while still leveraging the convenience of `fit()`. You can find more detailed information about the custom training loop at https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch. In this section, you will learn how to use the CRF layer in a custom training loop. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f90960e1c871"
      },
      "source": [
        "### Creating model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dcba022b4d6"
      },
      "source": [
        "Define BiLSTM+CRF model by using tfa.layers.CRF layer.\n",
        "The CRF layer not only ouput the CRF decode result (`decode_sequence`), but also outupt some interal variables (`potentials`, `sequence_length` and `kernel`). You will use those internal variables for compute loss value later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtylpxOmceaC"
      },
      "outputs": [],
      "source": [
        "# Build the model\n",
        "def build_embedding_bilstm_crf_model(\n",
        "    vocab_size: int, embed_dims: int, lstm_unit: int, tag_size: int\n",
        ") -> tf.keras.Model:\n",
        "    x = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name=\"x\")\n",
        "    y = tf.keras.layers.Embedding(vocab_size, embed_dims, mask_zero=True)(x)\n",
        "    y = tf.keras.layers.Bidirectional(\n",
        "        tf.keras.layers.LSTM(lstm_unit, return_sequences=True)\n",
        "    )(y)\n",
        "    decode_sequence, potentials, sequence_length, kernel = tfa.layers.CRF(tag_size)(y)\n",
        "\n",
        "    return tf.keras.Model(\n",
        "        inputs=x, outputs=[decode_sequence, potentials, sequence_length, kernel]\n",
        "    )\n",
        "\n",
        "\n",
        "model = build_embedding_bilstm_crf_model(VOCAB_SIZE, 32, 64, TAG_SIZE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwdM2pl3RSPb"
      },
      "source": [
        "Run the model on a single batch of data, and inspect the output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVMHNNbUXprr"
      },
      "outputs": [],
      "source": [
        "# preprocess\n",
        "preprecessd_tokens = preprecess_tokens(sample_tokens)\n",
        "\n",
        "# expand the tensor to shape: [1, None]. That is add batch dim\n",
        "inputs = tf.expand_dims(preprecessd_tokens, axis=0)\n",
        "\n",
        "outputs, *_ = model(inputs)\n",
        "print(outputs[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uabQmjMtRtzs"
      },
      "source": [
        "### Define CRF loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f54910109035"
      },
      "source": [
        "By using the real y and some internal variables of the CRF layer, you can compute the log likelihood of real y. The likelihood measures the probability of real y output from the CRF layer. This process can be done by using function tfa.text.crf_log_likelihood. Since the goal is making the model to output a higher likelihood of the real y, the negative of the log likelihood is used as the loss to optimize."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3b38225d9464"
      },
      "outputs": [],
      "source": [
        "def crf_loss_func(potentials, sequence_length, kernel, y):\n",
        "    crf_likelihood, _ = tfa.text.crf_log_likelihood(\n",
        "        potentials, y, sequence_length, kernel\n",
        "    )\n",
        "    # likelihood to loss\n",
        "    flat_crf_loss = -1 * crf_likelihood\n",
        "    crf_loss = tf.reduce_mean(flat_crf_loss)\n",
        "\n",
        "    return crf_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fb535bfc09d"
      },
      "source": [
        "### Define optimizer, metrics and train_step fucntion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U82B_tH2d294"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam(0.02)\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "\n",
        "@tf.function(experimental_relax_shapes=True)\n",
        "def train_step(x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        decoded_sequence, potentials, sequence_length, kernel = model(x)\n",
        "        crf_loss = crf_loss_func(potentials, sequence_length, kernel, y)\n",
        "        loss = crf_loss + tf.reduce_sum(model.losses)\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    train_loss(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70ecdcdf800d"
      },
      "source": [
        "### Training model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04b19e455b07"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    # Reset the metrics at the start of the next epoch\n",
        "    train_loss.reset_states()\n",
        "\n",
        "    for x, y in train_dataset:\n",
        "        train_step(x, y)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, \" f\"Loss: {train_loss.result()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c80d7afb8d0e"
      },
      "source": [
        "### Making inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8cc7e6acacf"
      },
      "source": [
        "Inspect the predicted result:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4xiJs1mi2yN"
      },
      "outputs": [],
      "source": [
        "# print the inputs and expected outputs\n",
        "print(\"raw inputs: \", sample_tokens)\n",
        "\n",
        "# preprocess\n",
        "preprocessed_inputs = preprecess_tokens(\n",
        "    sample_tokens\n",
        ")\n",
        "# expend the batch dim\n",
        "inputs = tf.reshape(preprocessed_inputs, shape=[1, -1])\n",
        "\n",
        "outputs, *_ = model.predict(inputs)\n",
        "prediction = [tags[i] for i in outputs[0]]\n",
        "\n",
        "# Keypoint: EU -> B-ORG, German -> B-MISC, British -> B-MISC\n",
        "print(\"ground true tags: \", sample_tags)\n",
        "print(\"predicted tags: \", prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_q1h8vQhMvKi"
      },
      "source": [
        "## Method two: Using the CRF layer via CRF model wrapper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eff89a7b6a0"
      },
      "source": [
        "Using a custom training loop is hard for junior TensorFlow developers, and it is error-prone (even for senior developers). TensorFlow Addons provides the CRF model wrapper to address this problem for most cases. You can find the API document at https://www.tensorflow.org/addons/api_docs/python/tfa/text/CRFModelWrapper. In this section, you will learn how to use the CRF model wrapper. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVuODB-cN9le"
      },
      "source": [
        "### Creating the base model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBqjfSFiN9lf"
      },
      "source": [
        "Define the BiLSTM model as the base model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_oMkjdoIN9lf"
      },
      "outputs": [],
      "source": [
        "# Build the model\n",
        "def build_embedding_bilstm_model(\n",
        "    vocab_size: int, embed_dims: int, lstm_unit: int\n",
        ") -> tf.keras.Model:\n",
        "    x = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name=\"x\")\n",
        "    y = tf.keras.layers.Embedding(vocab_size, embed_dims, mask_zero=True)(x)\n",
        "    y = tf.keras.layers.Bidirectional(\n",
        "        tf.keras.layers.LSTM(lstm_unit, return_sequences=True)\n",
        "    )(y)\n",
        "\n",
        "    return tf.keras.Model(\n",
        "        inputs=x, outputs=y\n",
        "    )\n",
        "\n",
        "\n",
        "base_model = build_embedding_bilstm_model(VOCAB_SIZE, 32, 64)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Gwcy02YN9lf"
      },
      "source": [
        "Run the model on a single batch of data, and inspect the output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-kBqrTxN9lf"
      },
      "outputs": [],
      "source": [
        "# preprocess\n",
        "preprecessd_tokens = preprecess_tokens(sample_tokens)\n",
        "\n",
        "# expand the tensor to shape: [1, None]. That is add batch dim\n",
        "inputs = tf.expand_dims(preprecessd_tokens, axis=0)\n",
        "\n",
        "outputs = base_model(inputs)\n",
        "print(outputs[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jg_qCpkOVLk"
      },
      "source": [
        "### CRF model wrapper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjBZDK5ojcSt"
      },
      "source": [
        "Let's import the CRF model wrapper (tfa.text.crf_wrapper.CRFModelWrapper) from TensorFlow Addons:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oySs_9WQNznV"
      },
      "outputs": [],
      "source": [
        "from tensorflow_addons.text.crf_wrapper import CRFModelWrapper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRY6ZrmmOjGC"
      },
      "source": [
        "### Wrapper base model with CRF model wrapper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CR2mcN14PIUC"
      },
      "source": [
        "The CRF model wrapper wraps the base model. It will apply the CRF layer to the output of the base model and compute the CRF loss. The wrapper takes the base model and number of tags (for initializing the CRF layer) as the initialization parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "joWipZyfOrix"
      },
      "outputs": [],
      "source": [
        "model = CRFModelWrapper(base_model, TAG_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSiBpyeVO_hg"
      },
      "source": [
        "### Training model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_5yFOJehGrr"
      },
      "source": [
        "The compilation of the wrappered model is exactly the same as that of the regular Keras model, except that there is no need to provide a loss function (the wrappered model computes the CRF loss internally)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PqGkCcdjhHWS"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(0.02))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFMeRQTrPI1B"
      },
      "outputs": [],
      "source": [
        "model.fit(train_dataset, epochs=10, verbose=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2s9PmHePzVr"
      },
      "source": [
        "### Making inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhsUyaOGPzVx"
      },
      "source": [
        "Inspect the predicted result:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpTr22-XPzVx"
      },
      "outputs": [],
      "source": [
        "# print the inputs and expected outputs\n",
        "print(\"raw inputs: \", sample_tokens)\n",
        "\n",
        "# preprocess\n",
        "preprocessed_inputs = preprecess_tokens(\n",
        "    sample_tokens\n",
        ")\n",
        "# expend the batch dim\n",
        "inputs = tf.reshape(preprocessed_inputs, shape=[1, -1])\n",
        "\n",
        "outputs = model.predict(inputs)\n",
        "prediction = [tags[i] for i in outputs[0]]\n",
        "\n",
        "# Keypoint: EU -> B-ORG, German -> B-MISC, British -> B-MISC\n",
        "print(\"ground true tags: \", sample_tags)\n",
        "print(\"predicted tags: \", prediction)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Tce3stUlHN0L"
      ],
      "name": "layers_crf.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
