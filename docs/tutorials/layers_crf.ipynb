{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2021 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "# TensorFlow Addons Layers: CRF\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/addons/tutorials/layers_crf\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/addons/blob/master/docs/tutorials/layers_crf.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/addons/blob/master/docs/tutorials/layers_crf.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "      <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/addons/docs/tutorials/layers_crf.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHxb-dlhMIzW"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook will demonstrate how to use the CRF (Conditional Random Field) layer in TensorFlow Addons.\n",
        "\n",
        "You will learn how to use the CRF layer in two ways by building NER models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUXex9ctTuDB"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEk-ibQkDNtF"
      },
      "outputs": [],
      "source": [
        "!pip install -q tensorflow-addons  # version >= 0.15.0 is required\n",
        "!pip install -q tensorflow\n",
        "!pip install -q datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqR2PQG4ZaZ0"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cb5bb13a189"
      },
      "source": [
        "## Traning data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3db2a84b25a7"
      },
      "source": [
        "Loading the CoNLL 2003 dataset by using the datasets library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WctVm30TFsT"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "de163e4e38bd427eaf47d68f2a2c91ad",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/2.60k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6fdcabb12f67460fbc8bb97f7bf0ec76",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.78k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading and preparing dataset conll2003/conll2003 (download: 4.63 MiB, generated: 9.78 MiB, post-processed: Unknown size, total: 14.41 MiB) to /root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fd97b2a29b394a3b950456899d899a68",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "21238e35687c4463bf58e856f2540497",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/650k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b09c6fd69e774f47b7be78cd930b47b0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/163k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c61a69600b6c48a6aff07cda07660a20",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/146k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "25ecd7491ea84f68bba4744b9499a588",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dfa9254232ba46178e58816c9a6fb58f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "caa83be48257474ebb6fc37ed0c7e567",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c3c0dea430b54516b44c14170632ddf6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset conll2003 downloaded and prepared to /root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6. Subsequent calls will reuse this data.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d0b6f281ad104ed5a415ad329100e3e9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "conll_data = datasets.load_dataset(\"conll2003\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUfW7yZyunU4"
      },
      "source": [
        "Inspect the data splits and features:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xh-sQGxGy-Tf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
              "        num_rows: 14041\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
              "        num_rows: 3250\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
              "        num_rows: 3453\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conll_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zsd8RqnjuvFh"
      },
      "source": [
        "Get a sample of train data and print it out:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8QLFEa2-TxhQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']\n",
            "[3, 0, 7, 0, 0, 0, 7, 0, 0]\n"
          ]
        }
      ],
      "source": [
        "for item in conll_data[\"train\"]:\n",
        "  sample_tokens = item['tokens']\n",
        "  sample_tag_ids = item[\"ner_tags\"]\n",
        "  print(sample_tokens)\n",
        "  print(sample_tag_ids)\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkCmPUPjvBC-"
      },
      "source": [
        "For our NER model, the input are the tokens which is a list of strings. The outputs are the NER tags which in the dataset they are the tag ids."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiRQdcVNvngI"
      },
      "source": [
        "The dataset also give the information about the mapping of NER tags and ids."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RugRp-kywNge"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n"
          ]
        }
      ],
      "source": [
        "dataset_builder = datasets.load_dataset_builder('conll2003')\n",
        "raw_tags = dataset_builder.info.features['ner_tags'].feature.names\n",
        "print(raw_tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eQvbO_Lv1KP"
      },
      "source": [
        "Let us decode the NER tag ids to tags."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_zH45qZwsSv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']\n",
            "['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']\n"
          ]
        }
      ],
      "source": [
        "sample_tags = [raw_tags[i] for i in sample_tag_ids]\n",
        "\n",
        "print(sample_tokens)\n",
        "print(sample_tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlXX1Qegv8-f"
      },
      "source": [
        "Those tags are used to encode the named entities by some format. In this dataset, tags are encoded in [IOB](https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)) format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBR_sqqAw4fA"
      },
      "source": [
        "Add a special tag `<PAD>` to the tag set which is used to represent a padding in the sequence. In NLP, 0 is usually used to mark padding. This is the default setting for many functions in Machine Learning software (include TensorFlow)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-67uGdIOzWDf"
      },
      "source": [
        "Create a list to convert tag ids to tag text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NChlWMnUTol"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['<PAD>', 'O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n"
          ]
        }
      ],
      "source": [
        "tags = ['<PAD>'] + raw_tags\n",
        "print(tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d45nlEDXWqG6"
      },
      "source": [
        "Define some constants which will be used in later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9A2UNwDnaPkX"
      },
      "outputs": [],
      "source": [
        "TAG_SIZE = len(tags)\n",
        "VOCAB_SIZE = 20000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8XXrsg1Wvvb"
      },
      "source": [
        "Building vocabulary lookup layer for tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3WJsqUBxpFX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20000\n",
            "['[MASK]', '[UNK]', 'the', '.', ',', 'of', 'in', 'to', 'a', 'and']\n"
          ]
        }
      ],
      "source": [
        "train_tokens = tf.ragged.constant(conll_data[\"train\"][\"tokens\"])\n",
        "train_tokens = tf.map_fn(tf.strings.lower, train_tokens)\n",
        "\n",
        "lookup_layer = tf.keras.layers.StringLookup(max_tokens=VOCAB_SIZE, mask_token=\"[MASK]\", oov_token=\"[UNK]\")\n",
        "lookup_layer.adapt(train_tokens)\n",
        "\n",
        "print(len(lookup_layer.get_vocabulary()))\n",
        "print(lookup_layer.get_vocabulary()[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lanszT8uXA6y"
      },
      "source": [
        "Creating raw (without preprocess) train and validation dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-u70kbtTSGg"
      },
      "outputs": [],
      "source": [
        "def create_data_generator(dataset):\n",
        "  def data_generator():\n",
        "    for item in dataset:\n",
        "      yield item['tokens'], item['ner_tags']\n",
        "  \n",
        "  return data_generator\n",
        "\n",
        "data_signature= (\n",
        "        tf.TensorSpec(shape=(None,), dtype=tf.string),\n",
        "        tf.TensorSpec(shape=(None, ), dtype=tf.int32)\n",
        ")\n",
        "\n",
        "train_data = tf.data.Dataset.from_generator(\n",
        "    create_data_generator(conll_data[\"train\"]),\n",
        "    output_signature=data_signature\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWuGTDeLXQ2D"
      },
      "source": [
        "Creating train and validation dataset that can be used for traning and validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EArTl8EBbCUF"
      },
      "outputs": [],
      "source": [
        "def dataset_preprocess(tokens, tag_ids):\n",
        "    preprocessed_tokens = preprecess_tokens(tokens)\n",
        "\n",
        "    # increase by 1 for all tag_ids,\n",
        "    # because `<PAD>` is added as the first element in tags list\n",
        "    preprocessed_tag_ids = tag_ids + 1\n",
        "\n",
        "    return preprocessed_tokens, preprocessed_tag_ids\n",
        "\n",
        "def preprecess_tokens(tokens):\n",
        "    tokens = tf.strings.lower(tokens)\n",
        "    return lookup_layer(tokens)\n",
        "\n",
        "BATCH_SIZE = 2048\n",
        "\n",
        "# With `padded_batch()`, each batch may have different length\n",
        "# shape: (batch_size, None)\n",
        "train_dataset = (\n",
        "    train_data.map(dataset_preprocess)\n",
        "    .padded_batch(batch_size=BATCH_SIZE).cache()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPjyvdqaMEhU"
      },
      "source": [
        "## Method one: Using the CRF layer in a custom training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f90960e1c871"
      },
      "source": [
        "### Creating model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dcba022b4d6"
      },
      "source": [
        "Define BiLSTM+CRF model by using tfa.layers.CRF layer.\n",
        "The CRF layer not only ouput the CRF decode result (`decode_sequence`), but also outupt some interal variables (`potentials`, `sequence_length` and `kernel`). You will use those internal variables for compute loss value later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtylpxOmceaC"
      },
      "outputs": [],
      "source": [
        "# Build the model\n",
        "def build_embedding_bilstm_crf_model(\n",
        "    vocab_size: int, embed_dims: int, lstm_unit: int, tag_size: int\n",
        ") -> tf.keras.Model:\n",
        "    x = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name=\"x\")\n",
        "    y = tf.keras.layers.Embedding(vocab_size, embed_dims, mask_zero=True)(x)\n",
        "    y = tf.keras.layers.Bidirectional(\n",
        "        tf.keras.layers.LSTM(lstm_unit, return_sequences=True)\n",
        "    )(y)\n",
        "    decode_sequence, potentials, sequence_length, kernel = tfa.layers.CRF(tag_size)(y)\n",
        "\n",
        "    return tf.keras.Model(\n",
        "        inputs=x, outputs=[decode_sequence, potentials, sequence_length, kernel]\n",
        "    )\n",
        "\n",
        "\n",
        "model = build_embedding_bilstm_crf_model(VOCAB_SIZE, 32, 64, TAG_SIZE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwdM2pl3RSPb"
      },
      "source": [
        "Run the model on a single batch of data, and inspect the output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVMHNNbUXprr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor([3 6 5 3 6 5 3 6 5], shape=(9,), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "# preprocess\n",
        "preprecessd_tokens = preprecess_tokens(sample_tokens)\n",
        "\n",
        "# expand the tensor to shape: [1, None]. That is add batch dim\n",
        "inputs = tf.expand_dims(preprecessd_tokens, axis=0)\n",
        "\n",
        "outputs, *_ = model(inputs)\n",
        "print(outputs[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uabQmjMtRtzs"
      },
      "source": [
        "### Define CRF loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f54910109035"
      },
      "source": [
        "By using the real y and some internal variables of the CRF layer. You can compute the log likelihood of real y. Use the negative of log likelihood as the loss to optimize."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3b38225d9464"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def crf_loss_func(potentials, sequence_length, kernel, y):\n",
        "    crf_likelihood, _ = tfa.text.crf_log_likelihood(\n",
        "        potentials, y, sequence_length, kernel\n",
        "    )\n",
        "    # likelihood to loss\n",
        "    flat_crf_loss = -1 * crf_likelihood\n",
        "    crf_loss = tf.reduce_mean(flat_crf_loss)\n",
        "\n",
        "    return crf_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fb535bfc09d"
      },
      "source": [
        "### Define optimizer, metrics and train_step fucntion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U82B_tH2d294"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam(0.02)\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "\n",
        "@tf.function(experimental_relax_shapes=True)\n",
        "def train_step(x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        decoded_sequence, potentials, sequence_length, kernel = model(x)\n",
        "        crf_loss = crf_loss_func(potentials, sequence_length, kernel, y)\n",
        "        loss = crf_loss + tf.reduce_sum(model.losses)\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    train_loss(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70ecdcdf800d"
      },
      "source": [
        "### Training model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04b19e455b07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 18.15883445739746\n",
            "Epoch 2, Loss: 8.863914489746094\n",
            "Epoch 3, Loss: 5.798288822174072\n",
            "Epoch 4, Loss: 4.2376203536987305\n",
            "Epoch 5, Loss: 3.174699068069458\n",
            "Epoch 6, Loss: 2.0829579830169678\n",
            "Epoch 7, Loss: 1.3333663940429688\n",
            "Epoch 8, Loss: 0.9537926912307739\n",
            "Epoch 9, Loss: 0.7511987090110779\n",
            "Epoch 10, Loss: 0.6272516250610352\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    # Reset the metrics at the start of the next epoch\n",
        "    train_loss.reset_states()\n",
        "\n",
        "    for x, y in train_dataset:\n",
        "        train_step(x, y)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, \" f\"Loss: {train_loss.result()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c80d7afb8d0e"
      },
      "source": [
        "### Making inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8cc7e6acacf"
      },
      "source": [
        "Inspect the predict result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4xiJs1mi2yN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "raw inputs:  ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']\n",
            "ground true tags:  ['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']\n",
            "predicted tags:  ['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']\n"
          ]
        }
      ],
      "source": [
        "# print the inputs and expected outputs\n",
        "print(\"raw inputs: \", sample_tokens)\n",
        "\n",
        "# preprocess\n",
        "preprocessed_inputs = preprecess_tokens(\n",
        "    sample_tokens\n",
        ")\n",
        "# expend the batch dim\n",
        "inputs = tf.reshape(preprocessed_inputs, shape=[1, -1])\n",
        "\n",
        "outputs, *_ = model.predict(inputs)\n",
        "prediction = [tags[i] for i in outputs[0]]\n",
        "\n",
        "# Keypoint: EU -> B-ORG, German -> B-MISC, British -> B-MISC\n",
        "print(\"ground true tags: \", sample_tags)\n",
        "print(\"predicted tags: \", prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_q1h8vQhMvKi"
      },
      "source": [
        "## Method two: Using the CRF layer via CRF model wrapper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVuODB-cN9le"
      },
      "source": [
        "### Creating the base model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBqjfSFiN9lf"
      },
      "source": [
        "Define the BiLSTM model as the base model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_oMkjdoIN9lf"
      },
      "outputs": [],
      "source": [
        "# Build the model\n",
        "def build_embedding_bilstm_crf_model(\n",
        "    vocab_size: int, embed_dims: int, lstm_unit: int\n",
        ") -> tf.keras.Model:\n",
        "    x = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name=\"x\")\n",
        "    y = tf.keras.layers.Embedding(vocab_size, embed_dims, mask_zero=True)(x)\n",
        "    y = tf.keras.layers.Bidirectional(\n",
        "        tf.keras.layers.LSTM(lstm_unit, return_sequences=True)\n",
        "    )(y)\n",
        "\n",
        "    return tf.keras.Model(\n",
        "        inputs=x, outputs=y\n",
        "    )\n",
        "\n",
        "\n",
        "base_model = build_embedding_bilstm_crf_model(VOCAB_SIZE, 32, 64)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Gwcy02YN9lf"
      },
      "source": [
        "Run the model on a single batch of data, and inspect the output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-kBqrTxN9lf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[-7.1890494e-03 -2.7290669e-03  4.7515053e-03 ... -8.8025322e-03\n",
            "  -4.7782061e-04  2.0253838e-03]\n",
            " [ 3.0374343e-03  7.1039307e-05  4.4372356e-03 ... -2.0702709e-03\n",
            "   8.3299953e-04 -1.6195974e-03]\n",
            " [ 7.4774139e-03  2.0081124e-03 -3.7772139e-04 ...  1.1178317e-03\n",
            "  -1.2569444e-03 -5.9724711e-03]\n",
            " ...\n",
            " [ 1.4448365e-03  1.6089321e-04  2.8041308e-04 ... -7.1640400e-04\n",
            "   1.8281980e-03  6.0155179e-04]\n",
            " [ 3.3373178e-03 -4.6984632e-03 -1.7297380e-03 ... -2.2621830e-03\n",
            "  -1.2909365e-03 -3.4608533e-05]\n",
            " [ 7.3946593e-03 -4.0191775e-03  3.0159338e-03 ... -3.6100592e-03\n",
            "  -1.1341731e-03 -2.9479943e-03]], shape=(9, 128), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "# preprocess\n",
        "preprecessd_tokens = preprecess_tokens(sample_tokens)\n",
        "\n",
        "# expand the tensor to shape: [1, None]. That is add batch dim\n",
        "inputs = tf.expand_dims(preprecessd_tokens, axis=0)\n",
        "\n",
        "outputs = base_model(inputs)\n",
        "print(outputs[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jg_qCpkOVLk"
      },
      "source": [
        "### CRF model wrapper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjBZDK5ojcSt"
      },
      "source": [
        "Import CRF model wrapper from TensorFlow Addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oySs_9WQNznV"
      },
      "outputs": [],
      "source": [
        "from tensorflow_addons.text.crf_wrapper import CRFModelWrapper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRY6ZrmmOjGC"
      },
      "source": [
        "### Wrapper base model with CRF model wrapper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CR2mcN14PIUC"
      },
      "source": [
        "The CRF model wrapper wraps the base model. It will apply the CRF layer to the output of the base model and compute the CRF loss. The wrapper takes the base model and number of tags (for initializing the CRF layer) as the initialization parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "joWipZyfOrix"
      },
      "outputs": [],
      "source": [
        "model = CRFModelWrapper(base_model, TAG_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSiBpyeVO_hg"
      },
      "source": [
        "### Traning model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_5yFOJehGrr"
      },
      "source": [
        "The compilation of the wrappered model is exactly the same as that of the regular Keras model, except that there is no need to provide a loss function (the wrappered model computes the CRF loss internally)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PqGkCcdjhHWS"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(0.02))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFMeRQTrPI1B"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "7/7 - 13s - loss: 10.5281 - crf_loss: 10.5281\n",
            "Epoch 2/10\n",
            "7/7 - 2s - loss: 7.7172 - crf_loss: 7.7172\n",
            "Epoch 3/10\n",
            "7/7 - 2s - loss: 5.1699 - crf_loss: 5.1699\n",
            "Epoch 4/10\n",
            "7/7 - 1s - loss: 4.0827 - crf_loss: 4.0827\n",
            "Epoch 5/10\n",
            "7/7 - 2s - loss: 2.9421 - crf_loss: 2.9421\n",
            "Epoch 6/10\n",
            "7/7 - 1s - loss: 1.8043 - crf_loss: 1.8043\n",
            "Epoch 7/10\n",
            "7/7 - 1s - loss: 1.2140 - crf_loss: 1.2140\n",
            "Epoch 8/10\n",
            "7/7 - 2s - loss: 0.8861 - crf_loss: 0.8861\n",
            "Epoch 9/10\n",
            "7/7 - 2s - loss: 0.6535 - crf_loss: 0.6535\n",
            "Epoch 10/10\n",
            "7/7 - 1s - loss: 0.5103 - crf_loss: 0.5103\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8639e039d0>"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(train_dataset, epochs=10, verbose=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2s9PmHePzVr"
      },
      "source": [
        "### Making inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhsUyaOGPzVx"
      },
      "source": [
        "Inspect the predict result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpTr22-XPzVx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "raw inputs:  ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']\n",
            "ground true tags:  ['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']\n",
            "predicted tags:  ['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']\n"
          ]
        }
      ],
      "source": [
        "# print the inputs and expected outputs\n",
        "print(\"raw inputs: \", sample_tokens)\n",
        "\n",
        "# preprocess\n",
        "preprocessed_inputs = preprecess_tokens(\n",
        "    sample_tokens\n",
        ")\n",
        "# expend the batch dim\n",
        "inputs = tf.reshape(preprocessed_inputs, shape=[1, -1])\n",
        "\n",
        "outputs = model.predict(inputs)\n",
        "prediction = [tags[i] for i in outputs[0]]\n",
        "\n",
        "# Keypoint: EU -> B-ORG, German -> B-MISC, British -> B-MISC\n",
        "print(\"ground true tags: \", sample_tags)\n",
        "print(\"predicted tags: \", prediction)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Tce3stUlHN0L"
      ],
      "name": "layers_crf.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
