{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "networks_seq2seq_nmt.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "source": [],
        "metadata": {
          "collapsed": false
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9ySOjrcc0Yp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bl9GdT7h0Hxk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhwgQAn50EZp",
        "colab_type": "text"
      },
      "source": [
        "# TensorFlow Addons Networks : Sequence-to-Sequence NMT with Attention Mechanism\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/addons/tutorials/networks_seq2seq_nmt\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/addons/blob/master/docs/tutorials/networks_seq2seq_nmt.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/addons/blob/master/docs/tutorials/networks_seq2seq_nmt.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "      <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/addons/docs/tutorials/networks_seq2seq_nmt.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ip0n8178Fuwm",
        "colab_type": "text"
      },
      "source": [
        "## Overview\n",
        "This notebook gives a brief introduction into the ***Sequence to Sequence Model Architecture***\n",
        "In this noteboook we broadly cover four essential topics necessary for Neural Machine Translation:\n",
        "\n",
        "\n",
        "* **Data cleaning**\n",
        "* **Data preparation**\n",
        "* **Neural Translation Model with Attention**\n",
        "* **Final Translation**\n",
        "\n",
        "The basic idea behind such a model though, is only the encoder-decoder architecture. These networks are usually used for a variety of tasks like text-summerization, Machine translation, Image Captioning, etc. This tutorial provideas a hands-on understanding of the concept, explaining the technical jargons wherever necessary. We focus on the task of Neural Machine Translation (NMT) which was the very first testbed for seq2seq models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNiadLKNLleD",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82GcQTsGf414",
        "colab_type": "text"
      },
      "source": [
        "## Additional Resources:\n",
        "\n",
        "These are a lst of resurces you must install in order to allow you to run this notebook:\n",
        "\n",
        "\n",
        "1. [German-English Dataset](http://www.manythings.org/anki/deu-eng.zip)\n",
        "\n",
        "\n",
        "The dataset should be downloaded, in order to compile this notebook, the embeddings can be used, as they are pretrained. Though, we carry out our own training here !!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OIlpST_6ga-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "b129799b-2108-437b-ca2f-c1973681cff6"
      },
      "source": [
        "#download data\n",
        "print(\"Downloading Dataset:\")\n",
        "!wget --quiet http://www.manythings.org/anki/deu-eng.zip\n",
        "!unzip deu-eng.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading Dataset:\n",
            "Archive:  deu-eng.zip\n",
            "  inflating: deu.txt                 \n",
            "  inflating: _about.txt              \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "co6-YpBwL-4d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "import string\n",
        "import re\n",
        "from typing import List, Tuple\n",
        "from pickle import dump\n",
        "from unicodedata import normalize\n",
        "import numpy as np\n",
        "import itertools\n",
        "from pickle import load\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from pickle import load\n",
        "import tensorflow as tf\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow_addons as tfa"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7gjUT_9XSoj",
        "colab_type": "text"
      },
      "source": [
        "## Data Cleaning\n",
        "\n",
        "Our data set is a German-English translation dataset. It contains 152,820 pairs of English to German phases, one pair per line with a tab separating the language. These dataset though organized needs cleaning before we can work on it. This will enable us to remove unnecessary bumps that may come in during the training. We also added start-of-sentence `<start>` and end-of-sentence `<end>` so that the model knows when to start and stop predicting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZIu-TNqKFsd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Start of sentence\n",
        "SOS = \"<start>\"\n",
        "# End of sentence\n",
        "EOS = \"<end>\"\n",
        "# Relevant punctuation\n",
        "PUNCTUATION = set(\"?,!.\")\n",
        "\n",
        "\n",
        "def load_dataset(filename: str) -> str:\n",
        "    \"\"\"\n",
        "    load dataset into memory\n",
        "    \"\"\"\n",
        "    with open(filename, mode=\"rt\", encoding=\"utf-8\") as fp:\n",
        "        return fp.read()\n",
        "\n",
        "\n",
        "def to_pairs(dataset: str, limit: int = None) -> List[Tuple[str, str]]:\n",
        "    \"\"\"\n",
        "    Split dataset into pairs of sentences, discards dataset line info.\n",
        "\n",
        "    e.g.\n",
        "    input -> 'Go.\\tGeh.\\tCC-BY 2.0 (France) Attribution: tatoeba.org\n",
        "    #2877272 (CM) & #8597805 (Roujin)'\n",
        "    output -> [('Go.', 'Geh.')]\n",
        "\n",
        "    :param dataset: dataset containing examples of translations between\n",
        "    two languages\n",
        "    the examples are delimited by `\\n` and the contents of the lines are\n",
        "    delimited by `\\t`\n",
        "    :param limit: number that limit dataset size (optional)\n",
        "    :return: list of pairs\n",
        "    \"\"\"\n",
        "    assert isinstance(limit, (int, None)), TypeError(\n",
        "        \"the limit value must be an integer\"\n",
        "    )\n",
        "    lines = dataset.strip().split(\"\\n\")\n",
        "    number_examples = limit or len(lines)  # if None get all\n",
        "    pairs = []\n",
        "    for line in lines[: abs(number_examples)]:\n",
        "        # take only source and target\n",
        "        src, trg, _ = line.split(\"\\t\")\n",
        "        pairs.append((src, trg))\n",
        "\n",
        "    # dataset size check\n",
        "    assert len(pairs) == number_examples\n",
        "    return pairs\n",
        "\n",
        "\n",
        "def separe_punctuation(token: str) -> str:\n",
        "    \"\"\"\n",
        "    Separe punctuation if exists\n",
        "    \"\"\"\n",
        "\n",
        "    if not set(token).intersection(PUNCTUATION):\n",
        "        return token\n",
        "    for p in PUNCTUATION:\n",
        "        token = f\" {p} \".join(token.split(p))\n",
        "    return \" \".join(token.split())\n",
        "\n",
        "\n",
        "def preprocess(sentence: str) -> str:\n",
        "    re_print = re.compile(f\"[^{re.escape(string.printable)}]\")\n",
        "    # convert lowercase and normalizing unicode characters\n",
        "    sentence = (\n",
        "        normalize(\"NFD\", sentence.lower()).encode(\"ascii\", \"ignore\").decode(\"UTF-8\")\n",
        "    )\n",
        "    cleaned_tokens = []\n",
        "    # tokenize sentence on white space\n",
        "    for token in sentence.split():\n",
        "        # removing non-printable chars form each token\n",
        "        token = re_print.sub(\"\", token).strip()\n",
        "        # ignore tokens with numbers\n",
        "        if re.findall(\"[0-9]\", token):\n",
        "            continue\n",
        "        # add space between words and punctuation eg: \"ok?go!\" => \"ok ? go !\"\n",
        "        token = separe_punctuation(token)\n",
        "        cleaned_tokens.append(token)\n",
        "\n",
        "    # rebuild sentence with space between tokens\n",
        "    sentence = \" \".join(cleaned_tokens)\n",
        "\n",
        "    # adding a start and an end token to the sentence\n",
        "    sentence = f\"{SOS} {sentence} {EOS}\"\n",
        "    return sentence\n",
        "\n",
        "\n",
        "def dataset_preprocess(dataset: List[Tuple[str, str]]) -> Tuple[List[str], List[str]]:\n",
        "    \"\"\"\n",
        "    - convert lowercase\n",
        "    - remove numbers\n",
        "    - remove special characters\n",
        "    - separe punctuation\n",
        "    - add start-of-sentence <start> and end-of-sentence <end>\n",
        "    \"\"\"\n",
        "    source_cleaned = []\n",
        "    target_cleaned = []\n",
        "    for source, target in dataset:\n",
        "        source_cleaned.append(preprocess(source))\n",
        "        target_cleaned.append(preprocess(target))\n",
        "    return source_cleaned, target_cleaned\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nDIELt9RH-w",
        "colab_type": "text"
      },
      "source": [
        "## Create Dataset\n",
        "\n",
        "- limit number of examples\n",
        "- load dataset into pairs `[('Be nice.', 'Seien Sie nett!'), ('Beat it.', 'Geh weg!'), ...]`\n",
        "- preprocessing dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMxdlVU1X8yI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "11f9397b-d556-4bfd-b9e0-42628093bd52"
      },
      "source": [
        "NUM_EXAMPLES = 10000 # Limit dataset size\n",
        "\n",
        "# load from .txt\n",
        "filename = 'deu.txt' #change filename if necessary\n",
        "dataset = load_dataset(filename)\n",
        "pairs = to_pairs(dataset, limit=NUM_EXAMPLES)\n",
        "print(f\"Dataset size: {len(pairs)}\")\n",
        "raw_data_en, raw_data_ge = dataset_preprocess(pairs)\n",
        "\n",
        "# show last 5 pairs\n",
        "for pair in zip(raw_data_en[-5:],raw_data_ge[-5:]):\n",
        "    print(pair)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset size: 10000\n",
            "(\"<start> tom's hungover . <end>\", '<start> tom ist verkatert . <end>')\n",
            "(\"<start> tom's in there . <end>\", '<start> tom ist da drinnen . <end>')\n",
            "(\"<start> tom's innocent . <end>\", '<start> tom ist unschuldig . <end>')\n",
            "(\"<start> tom's laughing . <end>\", '<start> tom lacht . <end>')\n",
            "(\"<start> tom's not busy . <end>\", '<start> tom ist nicht beschaftigt . <end>')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cfb66QxWYr6A",
        "colab_type": "text"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oq60MBPSanQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "en_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "en_tokenizer.fit_on_texts(raw_data_en)\n",
        "\n",
        "data_en = en_tokenizer.texts_to_sequences(raw_data_en)\n",
        "data_en = tf.keras.preprocessing.sequence.pad_sequences(data_en,padding='post')\n",
        "\n",
        "ge_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "ge_tokenizer.fit_on_texts(raw_data_ge)\n",
        "\n",
        "data_ge = ge_tokenizer.texts_to_sequences(raw_data_ge)\n",
        "data_ge = tf.keras.preprocessing.sequence.pad_sequences(data_ge,padding='post')"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH5oSRNeSc1s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def max_len(tensor):\n",
        "    #print( np.argmax([len(t) for t in tensor]))\n",
        "    return max( len(t) for t in tensor)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdM37lNBGXAj",
        "colab_type": "text"
      },
      "source": [
        "## Model Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfiBUJM2Et6C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train,  X_test, Y_train, Y_test = train_test_split(data_en,data_ge,test_size=0.2)\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = len(X_train)\n",
        "steps_per_epoch = BUFFER_SIZE//BATCH_SIZE\n",
        "embedding_dims = 256\n",
        "rnn_units = 1024\n",
        "dense_units = 1024\n",
        "Dtype = tf.float32   #used to initialize DecoderCell Zero state"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ff_jQHLhGqJU",
        "colab_type": "text"
      },
      "source": [
        "## Dataset Prepration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b__1hPHVFALO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f1673f7b-c4b3-4e0d-a946-4a1ddb365636"
      },
      "source": [
        "Tx = max_len(data_en)\n",
        "Ty = max_len(data_ge)  \n",
        "\n",
        "input_vocab_size = len(en_tokenizer.word_index)+1  \n",
        "output_vocab_size = len(ge_tokenizer.word_index)+ 1\n",
        "dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "example_X, example_Y = next(iter(dataset))\n",
        "print(example_X.shape) \n",
        "print(example_Y.shape) "
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 9)\n",
            "(64, 13)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQRgJcYgapqE",
        "colab_type": "text"
      },
      "source": [
        "## Defining NMT Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGdakRtjaokF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ENCODER\n",
        "class EncoderNetwork(tf.keras.Model):\n",
        "    def __init__(self,input_vocab_size,embedding_dims, rnn_units ):\n",
        "        super().__init__()\n",
        "        self.encoder_embedding = tf.keras.layers.Embedding(input_dim=input_vocab_size,\n",
        "                                                           output_dim=embedding_dims)\n",
        "        self.encoder_rnnlayer = tf.keras.layers.LSTM(rnn_units,return_sequences=True, \n",
        "                                                     return_state=True )\n",
        "    \n",
        "#DECODER\n",
        "class DecoderNetwork(tf.keras.Model):\n",
        "    def __init__(self,output_vocab_size, embedding_dims, rnn_units):\n",
        "        super().__init__()\n",
        "        self.decoder_embedding = tf.keras.layers.Embedding(input_dim=output_vocab_size,\n",
        "                                                           output_dim=embedding_dims) \n",
        "        self.dense_layer = tf.keras.layers.Dense(output_vocab_size)\n",
        "        self.decoder_rnncell = tf.keras.layers.LSTMCell(rnn_units)\n",
        "        # Sampler\n",
        "        self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
        "        # Create attention mechanism with memory = None\n",
        "        self.attention_mechanism = self.build_attention_mechanism(dense_units,None,BATCH_SIZE*[Tx])\n",
        "        self.rnn_cell =  self.build_rnn_cell(BATCH_SIZE)\n",
        "        self.decoder = tfa.seq2seq.BasicDecoder(self.rnn_cell, sampler= self.sampler,\n",
        "                                                output_layer=self.dense_layer)\n",
        "\n",
        "    def build_attention_mechanism(self, units,memory, memory_sequence_length):\n",
        "        return tfa.seq2seq.LuongAttention(units, memory = memory, \n",
        "                                          memory_sequence_length=memory_sequence_length)\n",
        "        #return tfa.seq2seq.BahdanauAttention(units, memory = memory, memory_sequence_length=memory_sequence_length)\n",
        "\n",
        "    # wrap decodernn cell  \n",
        "    def build_rnn_cell(self, batch_size ):\n",
        "        rnn_cell = tfa.seq2seq.AttentionWrapper(self.decoder_rnncell, self.attention_mechanism,\n",
        "                                                attention_layer_size=dense_units)\n",
        "        return rnn_cell\n",
        "    \n",
        "    def build_decoder_initial_state(self, batch_size, encoder_state,Dtype):\n",
        "        decoder_initial_state = self.rnn_cell.get_initial_state(batch_size = batch_size, \n",
        "                                                                dtype = Dtype)\n",
        "        decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state) \n",
        "        return decoder_initial_state\n",
        "\n",
        "encoderNetwork = EncoderNetwork(input_vocab_size,embedding_dims, rnn_units)\n",
        "decoderNetwork = DecoderNetwork(output_vocab_size,embedding_dims, rnn_units)\n",
        "optimizer = tf.keras.optimizers.Adam()\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPwcfddTa0oB",
        "colab_type": "text"
      },
      "source": [
        "## Initializing Training functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1BEqVyra2jW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_function(y_pred, y):\n",
        "   \n",
        "    #shape of y [batch_size, ty]\n",
        "    #shape of y_pred [batch_size, Ty, output_vocab_size] \n",
        "    sparsecategoricalcrossentropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
        "                                                                                  reduction='none')\n",
        "    loss = sparsecategoricalcrossentropy(y_true=y, y_pred=y_pred)\n",
        "    mask = tf.logical_not(tf.math.equal(y,0))   #output 0 for y=0 else output 1\n",
        "    mask = tf.cast(mask, dtype=loss.dtype)\n",
        "    loss = mask* loss\n",
        "    loss = tf.reduce_mean(loss)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def train_step(input_batch, output_batch,encoder_initial_cell_state):\n",
        "    #initialize loss = 0\n",
        "    loss = 0\n",
        "    with tf.GradientTape() as tape:\n",
        "        encoder_emb_inp = encoderNetwork.encoder_embedding(input_batch)\n",
        "        a, a_tx, c_tx = encoderNetwork.encoder_rnnlayer(encoder_emb_inp, \n",
        "                                                        initial_state =encoder_initial_cell_state)\n",
        "\n",
        "        #[last step activations,last memory_state] of encoder passed as input to decoder Network\n",
        "        \n",
        "         \n",
        "        # Prepare correct Decoder input & output sequence data\n",
        "        decoder_input = output_batch[:,:-1] # ignore <end>\n",
        "        #compare logits with timestepped +1 version of decoder_input\n",
        "        decoder_output = output_batch[:,1:] #ignore <start>\n",
        "\n",
        "\n",
        "        # Decoder Embeddings\n",
        "        decoder_emb_inp = decoderNetwork.decoder_embedding(decoder_input)\n",
        "\n",
        "        #Setting up decoder memory from encoder output and Zero State for AttentionWrapperState\n",
        "        decoderNetwork.attention_mechanism.setup_memory(a)\n",
        "        decoder_initial_state = decoderNetwork.build_decoder_initial_state(BATCH_SIZE,\n",
        "                                                                           encoder_state=[a_tx, c_tx],\n",
        "                                                                           Dtype=tf.float32)\n",
        "        \n",
        "        #BasicDecoderOutput        \n",
        "        outputs, _, _ = decoderNetwork.decoder(decoder_emb_inp,initial_state=decoder_initial_state,\n",
        "                                               sequence_length=BATCH_SIZE*[Ty-1])\n",
        "\n",
        "        logits = outputs.rnn_output\n",
        "        #Calculate loss\n",
        "\n",
        "        loss = loss_function(logits, decoder_output)\n",
        "\n",
        "    #Returns the list of all layer variables / weights.\n",
        "    variables = encoderNetwork.trainable_variables + decoderNetwork.trainable_variables  \n",
        "    # differentiate loss wrt variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "\n",
        "    #grads_and_vars â€“ List of(gradient, variable) pairs.\n",
        "    grads_and_vars = zip(gradients,variables)\n",
        "    optimizer.apply_gradients(grads_and_vars)\n",
        "    return loss"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71Lkdx6GFb3A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#RNN LSTM hidden and memory state initializer\n",
        "def initialize_initial_state():\n",
        "        return [tf.zeros((BATCH_SIZE, rnn_units)), tf.zeros((BATCH_SIZE, rnn_units))]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5uzLcu2bNX3",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvfD2SknWrt6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bb10dd51-8aa0-46e1-bad6-369ee175c671"
      },
      "source": [
        "epochs = 15\n",
        "for i in range(1, epochs+1):\n",
        "\n",
        "    encoder_initial_cell_state = initialize_initial_state()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for ( batch , (input_batch, output_batch)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        batch_loss = train_step(input_batch, output_batch, encoder_initial_cell_state)\n",
        "        total_loss += batch_loss\n",
        "        if (batch+1)%5 == 0:\n",
        "            print(\"total loss: {} epoch {} batch {} \".format(batch_loss.numpy(), i, batch+1))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total loss: 3.8943240642547607 epoch 1 batch 5 \n",
            "total loss: 2.772080183029175 epoch 1 batch 10 \n",
            "total loss: 2.38179087638855 epoch 1 batch 15 \n",
            "total loss: 2.212329387664795 epoch 1 batch 20 \n",
            "total loss: 2.213712453842163 epoch 1 batch 25 \n",
            "total loss: 2.008049726486206 epoch 1 batch 30 \n",
            "total loss: 1.901373028755188 epoch 1 batch 35 \n",
            "total loss: 1.9193779230117798 epoch 1 batch 40 \n",
            "total loss: 1.900277018547058 epoch 1 batch 45 \n",
            "total loss: 1.793951153755188 epoch 1 batch 50 \n",
            "total loss: 1.9367738962173462 epoch 1 batch 55 \n",
            "total loss: 1.7183160781860352 epoch 1 batch 60 \n",
            "total loss: 1.6661535501480103 epoch 1 batch 65 \n",
            "total loss: 1.642051100730896 epoch 1 batch 70 \n",
            "total loss: 1.6591120958328247 epoch 1 batch 75 \n",
            "total loss: 1.7424486875534058 epoch 1 batch 80 \n",
            "total loss: 1.573205590248108 epoch 1 batch 85 \n",
            "total loss: 1.6280765533447266 epoch 1 batch 90 \n",
            "total loss: 1.5122560262680054 epoch 1 batch 95 \n",
            "total loss: 1.5828932523727417 epoch 1 batch 100 \n",
            "total loss: 1.6335020065307617 epoch 1 batch 105 \n",
            "total loss: 1.5766042470932007 epoch 1 batch 110 \n",
            "total loss: 1.6154953241348267 epoch 1 batch 115 \n",
            "total loss: 1.4367307424545288 epoch 1 batch 120 \n",
            "total loss: 1.5218583345413208 epoch 1 batch 125 \n",
            "total loss: 1.4648972749710083 epoch 2 batch 5 \n",
            "total loss: 1.381431221961975 epoch 2 batch 10 \n",
            "total loss: 1.3941888809204102 epoch 2 batch 15 \n",
            "total loss: 1.40308678150177 epoch 2 batch 20 \n",
            "total loss: 1.4039493799209595 epoch 2 batch 25 \n",
            "total loss: 1.464238166809082 epoch 2 batch 30 \n",
            "total loss: 1.3346614837646484 epoch 2 batch 35 \n",
            "total loss: 1.284735083580017 epoch 2 batch 40 \n",
            "total loss: 1.3640228509902954 epoch 2 batch 45 \n",
            "total loss: 1.3634544610977173 epoch 2 batch 50 \n",
            "total loss: 1.4217529296875 epoch 2 batch 55 \n",
            "total loss: 1.5048540830612183 epoch 2 batch 60 \n",
            "total loss: 1.4030476808547974 epoch 2 batch 65 \n",
            "total loss: 1.3323160409927368 epoch 2 batch 70 \n",
            "total loss: 1.3493083715438843 epoch 2 batch 75 \n",
            "total loss: 1.3293452262878418 epoch 2 batch 80 \n",
            "total loss: 1.2715955972671509 epoch 2 batch 85 \n",
            "total loss: 1.3965988159179688 epoch 2 batch 90 \n",
            "total loss: 1.3759437799453735 epoch 2 batch 95 \n",
            "total loss: 1.2967075109481812 epoch 2 batch 100 \n",
            "total loss: 1.335363507270813 epoch 2 batch 105 \n",
            "total loss: 1.3633686304092407 epoch 2 batch 110 \n",
            "total loss: 1.2440804243087769 epoch 2 batch 115 \n",
            "total loss: 1.2360830307006836 epoch 2 batch 120 \n",
            "total loss: 1.2809593677520752 epoch 2 batch 125 \n",
            "total loss: 1.214463233947754 epoch 3 batch 5 \n",
            "total loss: 1.1408567428588867 epoch 3 batch 10 \n",
            "total loss: 1.0832160711288452 epoch 3 batch 15 \n",
            "total loss: 1.2230476140975952 epoch 3 batch 20 \n",
            "total loss: 1.1461988687515259 epoch 3 batch 25 \n",
            "total loss: 1.1558202505111694 epoch 3 batch 30 \n",
            "total loss: 1.1710516214370728 epoch 3 batch 35 \n",
            "total loss: 1.184714674949646 epoch 3 batch 40 \n",
            "total loss: 1.1288284063339233 epoch 3 batch 45 \n",
            "total loss: 1.129332423210144 epoch 3 batch 50 \n",
            "total loss: 1.1805278062820435 epoch 3 batch 55 \n",
            "total loss: 1.192665457725525 epoch 3 batch 60 \n",
            "total loss: 1.178153157234192 epoch 3 batch 65 \n",
            "total loss: 1.1328712701797485 epoch 3 batch 70 \n",
            "total loss: 1.0479586124420166 epoch 3 batch 75 \n",
            "total loss: 1.0495963096618652 epoch 3 batch 80 \n",
            "total loss: 1.081318974494934 epoch 3 batch 85 \n",
            "total loss: 1.1398619413375854 epoch 3 batch 90 \n",
            "total loss: 1.1488628387451172 epoch 3 batch 95 \n",
            "total loss: 1.1159776449203491 epoch 3 batch 100 \n",
            "total loss: 1.1572190523147583 epoch 3 batch 105 \n",
            "total loss: 1.166298508644104 epoch 3 batch 110 \n",
            "total loss: 1.0840826034545898 epoch 3 batch 115 \n",
            "total loss: 1.1529208421707153 epoch 3 batch 120 \n",
            "total loss: 1.1815710067749023 epoch 3 batch 125 \n",
            "total loss: 1.0111944675445557 epoch 4 batch 5 \n",
            "total loss: 1.002721905708313 epoch 4 batch 10 \n",
            "total loss: 0.9608625769615173 epoch 4 batch 15 \n",
            "total loss: 1.000160574913025 epoch 4 batch 20 \n",
            "total loss: 0.9396118521690369 epoch 4 batch 25 \n",
            "total loss: 0.9565573334693909 epoch 4 batch 30 \n",
            "total loss: 1.0375709533691406 epoch 4 batch 35 \n",
            "total loss: 0.9020416736602783 epoch 4 batch 40 \n",
            "total loss: 1.0192192792892456 epoch 4 batch 45 \n",
            "total loss: 0.9454962611198425 epoch 4 batch 50 \n",
            "total loss: 0.9689922332763672 epoch 4 batch 55 \n",
            "total loss: 0.94187992811203 epoch 4 batch 60 \n",
            "total loss: 0.8888412117958069 epoch 4 batch 65 \n",
            "total loss: 0.9316508769989014 epoch 4 batch 70 \n",
            "total loss: 1.002061128616333 epoch 4 batch 75 \n",
            "total loss: 0.9115805625915527 epoch 4 batch 80 \n",
            "total loss: 0.8875508308410645 epoch 4 batch 85 \n",
            "total loss: 0.9158795475959778 epoch 4 batch 90 \n",
            "total loss: 1.031103491783142 epoch 4 batch 95 \n",
            "total loss: 0.9380028247833252 epoch 4 batch 100 \n",
            "total loss: 0.9444494843482971 epoch 4 batch 105 \n",
            "total loss: 0.9645378589630127 epoch 4 batch 110 \n",
            "total loss: 0.807037889957428 epoch 4 batch 115 \n",
            "total loss: 1.0092644691467285 epoch 4 batch 120 \n",
            "total loss: 1.0061876773834229 epoch 4 batch 125 \n",
            "total loss: 0.7030258774757385 epoch 5 batch 5 \n",
            "total loss: 0.9186838269233704 epoch 5 batch 10 \n",
            "total loss: 0.8037793636322021 epoch 5 batch 15 \n",
            "total loss: 0.7768905758857727 epoch 5 batch 20 \n",
            "total loss: 0.7807180285453796 epoch 5 batch 25 \n",
            "total loss: 0.7432582974433899 epoch 5 batch 30 \n",
            "total loss: 0.6923568248748779 epoch 5 batch 35 \n",
            "total loss: 0.8769108653068542 epoch 5 batch 40 \n",
            "total loss: 0.7039246559143066 epoch 5 batch 45 \n",
            "total loss: 0.8025664687156677 epoch 5 batch 50 \n",
            "total loss: 0.7340380549430847 epoch 5 batch 55 \n",
            "total loss: 0.810374915599823 epoch 5 batch 60 \n",
            "total loss: 0.7377567291259766 epoch 5 batch 65 \n",
            "total loss: 0.7884548306465149 epoch 5 batch 70 \n",
            "total loss: 0.8067165017127991 epoch 5 batch 75 \n",
            "total loss: 0.7829656004905701 epoch 5 batch 80 \n",
            "total loss: 0.6907898783683777 epoch 5 batch 85 \n",
            "total loss: 0.7654430866241455 epoch 5 batch 90 \n",
            "total loss: 0.8076803088188171 epoch 5 batch 95 \n",
            "total loss: 0.7066188454627991 epoch 5 batch 100 \n",
            "total loss: 0.7626352906227112 epoch 5 batch 105 \n",
            "total loss: 0.8586174845695496 epoch 5 batch 110 \n",
            "total loss: 0.7645522952079773 epoch 5 batch 115 \n",
            "total loss: 0.8272983431816101 epoch 5 batch 120 \n",
            "total loss: 0.804262638092041 epoch 5 batch 125 \n",
            "total loss: 0.6087496280670166 epoch 6 batch 5 \n",
            "total loss: 0.5923413634300232 epoch 6 batch 10 \n",
            "total loss: 0.5446427464485168 epoch 6 batch 15 \n",
            "total loss: 0.569377601146698 epoch 6 batch 20 \n",
            "total loss: 0.5546544194221497 epoch 6 batch 25 \n",
            "total loss: 0.6121872067451477 epoch 6 batch 30 \n",
            "total loss: 0.6312928795814514 epoch 6 batch 35 \n",
            "total loss: 0.6838244795799255 epoch 6 batch 40 \n",
            "total loss: 0.713485062122345 epoch 6 batch 45 \n",
            "total loss: 0.6518018841743469 epoch 6 batch 50 \n",
            "total loss: 0.634179413318634 epoch 6 batch 55 \n",
            "total loss: 0.6996376514434814 epoch 6 batch 60 \n",
            "total loss: 0.6377868056297302 epoch 6 batch 65 \n",
            "total loss: 0.6376842856407166 epoch 6 batch 70 \n",
            "total loss: 0.6786300539970398 epoch 6 batch 75 \n",
            "total loss: 0.6189770102500916 epoch 6 batch 80 \n",
            "total loss: 0.6304643750190735 epoch 6 batch 85 \n",
            "total loss: 0.6242449879646301 epoch 6 batch 90 \n",
            "total loss: 0.575831949710846 epoch 6 batch 95 \n",
            "total loss: 0.6258266568183899 epoch 6 batch 100 \n",
            "total loss: 0.7544317245483398 epoch 6 batch 105 \n",
            "total loss: 0.7236025333404541 epoch 6 batch 110 \n",
            "total loss: 0.6269471645355225 epoch 6 batch 115 \n",
            "total loss: 0.654485285282135 epoch 6 batch 120 \n",
            "total loss: 0.6482054591178894 epoch 6 batch 125 \n",
            "total loss: 0.5321803689002991 epoch 7 batch 5 \n",
            "total loss: 0.44925642013549805 epoch 7 batch 10 \n",
            "total loss: 0.5164665579795837 epoch 7 batch 15 \n",
            "total loss: 0.6267779469490051 epoch 7 batch 20 \n",
            "total loss: 0.4595454931259155 epoch 7 batch 25 \n",
            "total loss: 0.5541160106658936 epoch 7 batch 30 \n",
            "total loss: 0.5288247466087341 epoch 7 batch 35 \n",
            "total loss: 0.4889428913593292 epoch 7 batch 40 \n",
            "total loss: 0.5303789377212524 epoch 7 batch 45 \n",
            "total loss: 0.5224104523658752 epoch 7 batch 50 \n",
            "total loss: 0.5561248660087585 epoch 7 batch 55 \n",
            "total loss: 0.44589856266975403 epoch 7 batch 60 \n",
            "total loss: 0.48845818638801575 epoch 7 batch 65 \n",
            "total loss: 0.5365614891052246 epoch 7 batch 70 \n",
            "total loss: 0.5435857176780701 epoch 7 batch 75 \n",
            "total loss: 0.45053598284721375 epoch 7 batch 80 \n",
            "total loss: 0.4396984577178955 epoch 7 batch 85 \n",
            "total loss: 0.5831804871559143 epoch 7 batch 90 \n",
            "total loss: 0.5184563398361206 epoch 7 batch 95 \n",
            "total loss: 0.5493617057800293 epoch 7 batch 100 \n",
            "total loss: 0.4977022409439087 epoch 7 batch 105 \n",
            "total loss: 0.5049214363098145 epoch 7 batch 110 \n",
            "total loss: 0.5112508535385132 epoch 7 batch 115 \n",
            "total loss: 0.47500672936439514 epoch 7 batch 120 \n",
            "total loss: 0.5271874666213989 epoch 7 batch 125 \n",
            "total loss: 0.3619574308395386 epoch 8 batch 5 \n",
            "total loss: 0.3099249601364136 epoch 8 batch 10 \n",
            "total loss: 0.3963419497013092 epoch 8 batch 15 \n",
            "total loss: 0.38025179505348206 epoch 8 batch 20 \n",
            "total loss: 0.3509727418422699 epoch 8 batch 25 \n",
            "total loss: 0.29164955019950867 epoch 8 batch 30 \n",
            "total loss: 0.3583029806613922 epoch 8 batch 35 \n",
            "total loss: 0.39728033542633057 epoch 8 batch 40 \n",
            "total loss: 0.3862229287624359 epoch 8 batch 45 \n",
            "total loss: 0.4778132438659668 epoch 8 batch 50 \n",
            "total loss: 0.4147240221500397 epoch 8 batch 55 \n",
            "total loss: 0.39208778738975525 epoch 8 batch 60 \n",
            "total loss: 0.40576091408729553 epoch 8 batch 65 \n",
            "total loss: 0.44903233647346497 epoch 8 batch 70 \n",
            "total loss: 0.40688443183898926 epoch 8 batch 75 \n",
            "total loss: 0.44500279426574707 epoch 8 batch 80 \n",
            "total loss: 0.390073299407959 epoch 8 batch 85 \n",
            "total loss: 0.44843772053718567 epoch 8 batch 90 \n",
            "total loss: 0.4034801423549652 epoch 8 batch 95 \n",
            "total loss: 0.49192652106285095 epoch 8 batch 100 \n",
            "total loss: 0.41387295722961426 epoch 8 batch 105 \n",
            "total loss: 0.39322447776794434 epoch 8 batch 110 \n",
            "total loss: 0.5095522403717041 epoch 8 batch 115 \n",
            "total loss: 0.42950379848480225 epoch 8 batch 120 \n",
            "total loss: 0.5076374411582947 epoch 8 batch 125 \n",
            "total loss: 0.2784596383571625 epoch 9 batch 5 \n",
            "total loss: 0.2817988097667694 epoch 9 batch 10 \n",
            "total loss: 0.27460625767707825 epoch 9 batch 15 \n",
            "total loss: 0.28749534487724304 epoch 9 batch 20 \n",
            "total loss: 0.2654282748699188 epoch 9 batch 25 \n",
            "total loss: 0.3532451093196869 epoch 9 batch 30 \n",
            "total loss: 0.3095969259738922 epoch 9 batch 35 \n",
            "total loss: 0.362971693277359 epoch 9 batch 40 \n",
            "total loss: 0.3234662413597107 epoch 9 batch 45 \n",
            "total loss: 0.3249295949935913 epoch 9 batch 50 \n",
            "total loss: 0.3430158197879791 epoch 9 batch 55 \n",
            "total loss: 0.32500410079956055 epoch 9 batch 60 \n",
            "total loss: 0.3624382019042969 epoch 9 batch 65 \n",
            "total loss: 0.29069265723228455 epoch 9 batch 70 \n",
            "total loss: 0.3616417348384857 epoch 9 batch 75 \n",
            "total loss: 0.3826664686203003 epoch 9 batch 80 \n",
            "total loss: 0.41604506969451904 epoch 9 batch 85 \n",
            "total loss: 0.3346388638019562 epoch 9 batch 90 \n",
            "total loss: 0.34562602639198303 epoch 9 batch 95 \n",
            "total loss: 0.33310720324516296 epoch 9 batch 100 \n",
            "total loss: 0.37826645374298096 epoch 9 batch 105 \n",
            "total loss: 0.29235926270484924 epoch 9 batch 110 \n",
            "total loss: 0.3560928404331207 epoch 9 batch 115 \n",
            "total loss: 0.39650675654411316 epoch 9 batch 120 \n",
            "total loss: 0.3685692250728607 epoch 9 batch 125 \n",
            "total loss: 0.25923192501068115 epoch 10 batch 5 \n",
            "total loss: 0.2185114622116089 epoch 10 batch 10 \n",
            "total loss: 0.2717999219894409 epoch 10 batch 15 \n",
            "total loss: 0.2284327745437622 epoch 10 batch 20 \n",
            "total loss: 0.3001408576965332 epoch 10 batch 25 \n",
            "total loss: 0.31664636731147766 epoch 10 batch 30 \n",
            "total loss: 0.30952998995780945 epoch 10 batch 35 \n",
            "total loss: 0.28323692083358765 epoch 10 batch 40 \n",
            "total loss: 0.2964726984500885 epoch 10 batch 45 \n",
            "total loss: 0.30666670203208923 epoch 10 batch 50 \n",
            "total loss: 0.23124681413173676 epoch 10 batch 55 \n",
            "total loss: 0.32344701886177063 epoch 10 batch 60 \n",
            "total loss: 0.30215752124786377 epoch 10 batch 65 \n",
            "total loss: 0.33824780583381653 epoch 10 batch 70 \n",
            "total loss: 0.274232417345047 epoch 10 batch 75 \n",
            "total loss: 0.31185486912727356 epoch 10 batch 80 \n",
            "total loss: 0.284210741519928 epoch 10 batch 85 \n",
            "total loss: 0.27280303835868835 epoch 10 batch 90 \n",
            "total loss: 0.27922502160072327 epoch 10 batch 95 \n",
            "total loss: 0.30908241868019104 epoch 10 batch 100 \n",
            "total loss: 0.31311050057411194 epoch 10 batch 105 \n",
            "total loss: 0.34780192375183105 epoch 10 batch 110 \n",
            "total loss: 0.2594057023525238 epoch 10 batch 115 \n",
            "total loss: 0.2975548803806305 epoch 10 batch 120 \n",
            "total loss: 0.3586268723011017 epoch 10 batch 125 \n",
            "total loss: 0.18657004833221436 epoch 11 batch 5 \n",
            "total loss: 0.19782030582427979 epoch 11 batch 10 \n",
            "total loss: 0.2478076070547104 epoch 11 batch 15 \n",
            "total loss: 0.22392261028289795 epoch 11 batch 20 \n",
            "total loss: 0.16224820911884308 epoch 11 batch 25 \n",
            "total loss: 0.18594129383563995 epoch 11 batch 30 \n",
            "total loss: 0.2407170981168747 epoch 11 batch 35 \n",
            "total loss: 0.2693817913532257 epoch 11 batch 40 \n",
            "total loss: 0.18802666664123535 epoch 11 batch 45 \n",
            "total loss: 0.241646870970726 epoch 11 batch 50 \n",
            "total loss: 0.2317349761724472 epoch 11 batch 55 \n",
            "total loss: 0.2343464344739914 epoch 11 batch 60 \n",
            "total loss: 0.21849189698696136 epoch 11 batch 65 \n",
            "total loss: 0.28091323375701904 epoch 11 batch 70 \n",
            "total loss: 0.23119670152664185 epoch 11 batch 75 \n",
            "total loss: 0.22884933650493622 epoch 11 batch 80 \n",
            "total loss: 0.2005271315574646 epoch 11 batch 85 \n",
            "total loss: 0.22750918567180634 epoch 11 batch 90 \n",
            "total loss: 0.2657736837863922 epoch 11 batch 95 \n",
            "total loss: 0.24240249395370483 epoch 11 batch 100 \n",
            "total loss: 0.28540703654289246 epoch 11 batch 105 \n",
            "total loss: 0.2204034924507141 epoch 11 batch 110 \n",
            "total loss: 0.29181763529777527 epoch 11 batch 115 \n",
            "total loss: 0.3284190595149994 epoch 11 batch 120 \n",
            "total loss: 0.2810553014278412 epoch 11 batch 125 \n",
            "total loss: 0.14715522527694702 epoch 12 batch 5 \n",
            "total loss: 0.20515663921833038 epoch 12 batch 10 \n",
            "total loss: 0.18476448953151703 epoch 12 batch 15 \n",
            "total loss: 0.18449126183986664 epoch 12 batch 20 \n",
            "total loss: 0.21469910442829132 epoch 12 batch 25 \n",
            "total loss: 0.15620894730091095 epoch 12 batch 30 \n",
            "total loss: 0.16494204103946686 epoch 12 batch 35 \n",
            "total loss: 0.1933702677488327 epoch 12 batch 40 \n",
            "total loss: 0.2208998054265976 epoch 12 batch 45 \n",
            "total loss: 0.20796512067317963 epoch 12 batch 50 \n",
            "total loss: 0.20989835262298584 epoch 12 batch 55 \n",
            "total loss: 0.2064664363861084 epoch 12 batch 60 \n",
            "total loss: 0.2563227713108063 epoch 12 batch 65 \n",
            "total loss: 0.253417432308197 epoch 12 batch 70 \n",
            "total loss: 0.23944437503814697 epoch 12 batch 75 \n",
            "total loss: 0.18791396915912628 epoch 12 batch 80 \n",
            "total loss: 0.23468048870563507 epoch 12 batch 85 \n",
            "total loss: 0.21857166290283203 epoch 12 batch 90 \n",
            "total loss: 0.2546730041503906 epoch 12 batch 95 \n",
            "total loss: 0.22558915615081787 epoch 12 batch 100 \n",
            "total loss: 0.23694880306720734 epoch 12 batch 105 \n",
            "total loss: 0.2721855938434601 epoch 12 batch 110 \n",
            "total loss: 0.24095071852207184 epoch 12 batch 115 \n",
            "total loss: 0.2390017956495285 epoch 12 batch 120 \n",
            "total loss: 0.28833600878715515 epoch 12 batch 125 \n",
            "total loss: 0.15568971633911133 epoch 13 batch 5 \n",
            "total loss: 0.1386902630329132 epoch 13 batch 10 \n",
            "total loss: 0.17051170766353607 epoch 13 batch 15 \n",
            "total loss: 0.18466120958328247 epoch 13 batch 20 \n",
            "total loss: 0.1692444235086441 epoch 13 batch 25 \n",
            "total loss: 0.13866065442562103 epoch 13 batch 30 \n",
            "total loss: 0.1708802729845047 epoch 13 batch 35 \n",
            "total loss: 0.20048575103282928 epoch 13 batch 40 \n",
            "total loss: 0.15466105937957764 epoch 13 batch 45 \n",
            "total loss: 0.17185275256633759 epoch 13 batch 50 \n",
            "total loss: 0.1915184110403061 epoch 13 batch 55 \n",
            "total loss: 0.19304363429546356 epoch 13 batch 60 \n",
            "total loss: 0.19129277765750885 epoch 13 batch 65 \n",
            "total loss: 0.14907591044902802 epoch 13 batch 70 \n",
            "total loss: 0.19821268320083618 epoch 13 batch 75 \n",
            "total loss: 0.19735586643218994 epoch 13 batch 80 \n",
            "total loss: 0.23307891190052032 epoch 13 batch 85 \n",
            "total loss: 0.24129635095596313 epoch 13 batch 90 \n",
            "total loss: 0.1898774355649948 epoch 13 batch 95 \n",
            "total loss: 0.18202131986618042 epoch 13 batch 100 \n",
            "total loss: 0.19145013391971588 epoch 13 batch 105 \n",
            "total loss: 0.18047769367694855 epoch 13 batch 110 \n",
            "total loss: 0.28911110758781433 epoch 13 batch 115 \n",
            "total loss: 0.25191518664360046 epoch 13 batch 120 \n",
            "total loss: 0.22250919044017792 epoch 13 batch 125 \n",
            "total loss: 0.16623173654079437 epoch 14 batch 5 \n",
            "total loss: 0.15881699323654175 epoch 14 batch 10 \n",
            "total loss: 0.14264267683029175 epoch 14 batch 15 \n",
            "total loss: 0.12685339152812958 epoch 14 batch 20 \n",
            "total loss: 0.16715379059314728 epoch 14 batch 25 \n",
            "total loss: 0.14499446749687195 epoch 14 batch 30 \n",
            "total loss: 0.13823820650577545 epoch 14 batch 35 \n",
            "total loss: 0.1707833856344223 epoch 14 batch 40 \n",
            "total loss: 0.14757628738880157 epoch 14 batch 45 \n",
            "total loss: 0.18516135215759277 epoch 14 batch 50 \n",
            "total loss: 0.1569213718175888 epoch 14 batch 55 \n",
            "total loss: 0.13301672041416168 epoch 14 batch 60 \n",
            "total loss: 0.12995241582393646 epoch 14 batch 65 \n",
            "total loss: 0.16548241674900055 epoch 14 batch 70 \n",
            "total loss: 0.1881595402956009 epoch 14 batch 75 \n",
            "total loss: 0.18523018062114716 epoch 14 batch 80 \n",
            "total loss: 0.19846534729003906 epoch 14 batch 85 \n",
            "total loss: 0.19888122379779816 epoch 14 batch 90 \n",
            "total loss: 0.1881190687417984 epoch 14 batch 95 \n",
            "total loss: 0.1744716614484787 epoch 14 batch 100 \n",
            "total loss: 0.2092258483171463 epoch 14 batch 105 \n",
            "total loss: 0.23246914148330688 epoch 14 batch 110 \n",
            "total loss: 0.20477275550365448 epoch 14 batch 115 \n",
            "total loss: 0.1830405443906784 epoch 14 batch 120 \n",
            "total loss: 0.2035452276468277 epoch 14 batch 125 \n",
            "total loss: 0.13492198288440704 epoch 15 batch 5 \n",
            "total loss: 0.13279913365840912 epoch 15 batch 10 \n",
            "total loss: 0.09664832800626755 epoch 15 batch 15 \n",
            "total loss: 0.14865654706954956 epoch 15 batch 20 \n",
            "total loss: 0.17064577341079712 epoch 15 batch 25 \n",
            "total loss: 0.18528850376605988 epoch 15 batch 30 \n",
            "total loss: 0.1560884714126587 epoch 15 batch 35 \n",
            "total loss: 0.12524652481079102 epoch 15 batch 40 \n",
            "total loss: 0.13100281357765198 epoch 15 batch 45 \n",
            "total loss: 0.16509339213371277 epoch 15 batch 50 \n",
            "total loss: 0.13845351338386536 epoch 15 batch 55 \n",
            "total loss: 0.1652478128671646 epoch 15 batch 60 \n",
            "total loss: 0.16157875955104828 epoch 15 batch 65 \n",
            "total loss: 0.1522998958826065 epoch 15 batch 70 \n",
            "total loss: 0.16253794729709625 epoch 15 batch 75 \n",
            "total loss: 0.19812659919261932 epoch 15 batch 80 \n",
            "total loss: 0.1460370272397995 epoch 15 batch 85 \n",
            "total loss: 0.1395939737558365 epoch 15 batch 90 \n",
            "total loss: 0.19202987849712372 epoch 15 batch 95 \n",
            "total loss: 0.2335830181837082 epoch 15 batch 100 \n",
            "total loss: 0.18647915124893188 epoch 15 batch 105 \n",
            "total loss: 0.22846125066280365 epoch 15 batch 110 \n",
            "total loss: 0.19059689342975616 epoch 15 batch 115 \n",
            "total loss: 0.16890309751033783 epoch 15 batch 120 \n",
            "total loss: 0.18568718433380127 epoch 15 batch 125 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDyK-EGqbN5r",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y98sfom7SuGy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "01f486dd-5405-4b60-a3ca-94a957269444"
      },
      "source": [
        "#In this section we evaluate our model on a raw_input converted to german, for this the entire sentence has to be passed\n",
        "#through the length of the model, for this we use greedsampler to run through the decoder\n",
        "#and the final embedding matrix trained on the data is used to generate embeddings\n",
        "input_raw='how are you'\n",
        "\n",
        "# We have a transcript file containing English-German pairs\n",
        "# Preprocess X\n",
        "input_lines = ['<start> '+input_raw+'']\n",
        "input_sequences = [[en_tokenizer.word_index[w] for w in line.split(' ')] for line in input_lines]\n",
        "input_sequences = tf.keras.preprocessing.sequence.pad_sequences(input_sequences,\n",
        "                                                                maxlen=Tx, padding='post')\n",
        "inp = tf.convert_to_tensor(input_sequences)\n",
        "#print(inp.shape)\n",
        "inference_batch_size = input_sequences.shape[0]\n",
        "encoder_initial_cell_state = [tf.zeros((inference_batch_size, rnn_units)),\n",
        "                              tf.zeros((inference_batch_size, rnn_units))]\n",
        "encoder_emb_inp = encoderNetwork.encoder_embedding(inp)\n",
        "a, a_tx, c_tx = encoderNetwork.encoder_rnnlayer(encoder_emb_inp,\n",
        "                                                initial_state =encoder_initial_cell_state)\n",
        "print('a_tx :',a_tx.shape)\n",
        "print('c_tx :', c_tx.shape)\n",
        "\n",
        "start_tokens = tf.fill([inference_batch_size],ge_tokenizer.word_index['<start>'])\n",
        "\n",
        "end_token = ge_tokenizer.word_index['<end>']\n",
        "\n",
        "greedy_sampler = tfa.seq2seq.GreedyEmbeddingSampler()\n",
        "\n",
        "decoder_input = tf.expand_dims([ge_tokenizer.word_index['<start>']]* inference_batch_size,1)\n",
        "decoder_emb_inp = decoderNetwork.decoder_embedding(decoder_input)\n",
        "\n",
        "decoder_instance = tfa.seq2seq.BasicDecoder(cell = decoderNetwork.rnn_cell, sampler = greedy_sampler,\n",
        "                                            output_layer=decoderNetwork.dense_layer)\n",
        "decoderNetwork.attention_mechanism.setup_memory(a)\n",
        "#pass [ last step activations , encoder memory_state ] as input to decoder for LSTM\n",
        "print(\"decoder_initial_state = [a_tx, c_tx] :\",np.array([a_tx, c_tx]).shape)\n",
        "decoder_initial_state = decoderNetwork.build_decoder_initial_state(inference_batch_size,\n",
        "                                                                   encoder_state=[a_tx, c_tx],\n",
        "                                                                   Dtype=tf.float32)\n",
        "print(\"\\nCompared to simple encoder-decoder without attention, the decoder_initial_state \\\n",
        " is an AttentionWrapperState object containing s_prev tensors and context and alignment vector \\n \")\n",
        "print(\"decoder initial state shape :\",np.array(decoder_initial_state).shape)\n",
        "print(\"decoder_initial_state tensor \\n\", decoder_initial_state)\n",
        "\n",
        "# Since we do not know the target sequence lengths in advance, we use maximum_iterations to limit the translation lengths.\n",
        "# One heuristic is to decode up to two times the source sentence lengths.\n",
        "maximum_iterations = tf.round(tf.reduce_max(Tx) * 2)\n",
        "\n",
        "#initialize inference decoder\n",
        "decoder_embedding_matrix = decoderNetwork.decoder_embedding.variables[0] \n",
        "(first_finished, first_inputs,first_state) = decoder_instance.initialize(decoder_embedding_matrix,\n",
        "                             start_tokens = start_tokens,\n",
        "                             end_token=end_token,\n",
        "                             initial_state = decoder_initial_state)\n",
        "#print( first_finished.shape)\n",
        "print(\"\\nfirst_inputs returns the same decoder_input i.e. embedding of  <start> :\",first_inputs.shape)\n",
        "print(\"start_index_emb_avg \", tf.reduce_sum(tf.reduce_mean(first_inputs, axis=0))) # mean along the batch\n",
        "\n",
        "inputs = first_inputs\n",
        "state = first_state  \n",
        "predictions = np.empty((inference_batch_size,0), dtype = np.int32)                                                                             \n",
        "for j in range(maximum_iterations):\n",
        "    outputs, next_state, next_inputs, finished = decoder_instance.step(j,inputs,state)\n",
        "    inputs = next_inputs\n",
        "    state = next_state\n",
        "    outputs = np.expand_dims(outputs.sample_id,axis = -1)\n",
        "    predictions = np.append(predictions, outputs, axis = -1)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a_tx : (1, 1024)\n",
            "c_tx : (1, 1024)\n",
            "decoder_initial_state = [a_tx, c_tx] : (2, 1, 1024)\n",
            "\n",
            "Compared to simple encoder-decoder without attention, the decoder_initial_state  is an AttentionWrapperState object containing s_prev tensors and context and alignment vector \n",
            " \n",
            "decoder initial state shape : (6,)\n",
            "decoder_initial_state tensor \n",
            " AttentionWrapperState(cell_state=[<tf.Tensor: shape=(1, 1024), dtype=float32, numpy=\n",
            "array([[-0.01045269,  0.00690599,  0.03041662, ..., -0.00491909,\n",
            "        -0.04608576,  0.00574572]], dtype=float32)>, <tf.Tensor: shape=(1, 1024), dtype=float32, numpy=\n",
            "array([[-0.02426171,  0.01850663,  0.09925612, ..., -0.01409284,\n",
            "        -0.20349137,  0.02523274]], dtype=float32)>], attention=<tf.Tensor: shape=(1, 1024), dtype=float32, numpy=array([[0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>, time=<tf.Tensor: shape=(), dtype=int32, numpy=0>, alignments=<tf.Tensor: shape=(1, 9), dtype=float32, numpy=array([[0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)>, alignment_history=(), attention_state=<tf.Tensor: shape=(1, 9), dtype=float32, numpy=array([[0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)>)\n",
            "\n",
            "first_inputs returns the same decoder_input i.e. embedding of  <start> : (1, 256)\n",
            "start_index_emb_avg  tf.Tensor(0.08019361, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iodjSItQds1t",
        "colab_type": "text"
      },
      "source": [
        "## Final Translation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6aWFB5IWlH2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "42608fdd-198f-42db-9efc-94c4c828c383"
      },
      "source": [
        "#prediction based on our sentence earlier\n",
        "print(\"English Sentence:\")\n",
        "print(input_raw)\n",
        "print(\"\\nGerman Translation:\")\n",
        "for i in range(len(predictions)):\n",
        "    line = predictions[i,:]\n",
        "    seq = list(itertools.takewhile( lambda index: index !=2, line))\n",
        "    print(\" \".join( [ge_tokenizer.index_word[w] for w in seq]))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Sentence:\n",
            "how are you\n",
            "\n",
            "German Translation:\n",
            "wie bist du .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6Av-oPWvRc4",
        "colab_type": "text"
      },
      "source": [
        "### The accuracy can be improved by implementing:\n",
        "* Beam Search or Lexicon Search\n",
        "* Bi-directional encoder-decoder model "
      ]
    }
  ]
}