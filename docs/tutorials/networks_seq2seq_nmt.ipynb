{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "f9ySOjrcc0Yp"
      },
      "outputs": [],
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "bl9GdT7h0Hxk"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WhwgQAn50EZp"
      },
      "source": [
        "# TensorFlow Addons Networks : Sequence-to-Sequence NMT with Attention Mechanism\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/addons/tutorials/networks_seq2seq_nmt\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/addons/blob/master/docs/tutorials/networks_seq2seq_nmt.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/addons/blob/master/docs/tutorials/networks_seq2seq_nmt.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "      <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/addons/docs/tutorials/networks_seq2seq_nmt.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ip0n8178Fuwm"
      },
      "source": [
        "## Overview\n",
        "This notebook gives a brief introduction into the ***Sequence to Sequence Model Architecture***\n",
        "In this noteboook we broadly cover four essential topics necessary for Neural Machine Translation:\n",
        "\n",
        "\n",
        "* **Data cleaning**\n",
        "* **Data preparation**\n",
        "* **Neural Translation Model with Attention**\n",
        "* **Final Translation**\n",
        "\n",
        "The basic idea behind such a model though, is only the encoder-decoder architecture. These networks are usually used for a variety of tasks like text-summerization, Machine translation, Image Captioning, etc. This tutorial provideas a hands-on understanding of the concept, explaining the technical jargons wherever necessary. We focus on the task of Neural Machine Translation (NMT) which was the very first testbed for seq2seq models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YNiadLKNLleD"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "82GcQTsGf414"
      },
      "source": [
        "## Additional Resources:\n",
        "\n",
        "These are a lst of resurces you must install in order to allow you to run this notebook:\n",
        "\n",
        "\n",
        "1. [German-English Dataset](http://www.manythings.org/anki/deu-eng.zip)\n",
        "\n",
        "\n",
        "The dataset should be downloaded, in order to compile this notebook, the embeddings can be used, as they are pretrained. Though, we carry out our own training here.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -U tensorflow-addons\n",
        "!pip install nltk sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "5OIlpST_6ga-"
      },
      "outputs": [],
      "source": [
        "#download data\n",
        "print(\"Downloading Dataset:\")\n",
        "!wget --quiet http://www.manythings.org/anki/deu-eng.zip\n",
        "!unzip -o deu-eng.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "co6-YpBwL-4d",
        "outputId": "6571961c-8f50-4333-9b1d-5eb1a157f4f8"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import string\n",
        "import re\n",
        "from typing import List, Tuple\n",
        "from pickle import dump\n",
        "from unicodedata import normalize\n",
        "import numpy as np\n",
        "import itertools\n",
        "from pickle import load\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from pickle import load\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow_addons as tfa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q7gjUT_9XSoj"
      },
      "source": [
        "## Data Cleaning\n",
        "\n",
        "Our data set is a German-English translation dataset. It contains 152,820 pairs of English to German phases, one pair per line with a tab separating the language. These dataset though organized needs cleaning before we can work on it. This will enable us to remove unnecessary bumps that may come in during the training. We also added start-of-sentence `<start>` and end-of-sentence `<end>` so that the model knows when to start and stop predicting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "6ZIu-TNqKFsd"
      },
      "outputs": [],
      "source": [
        "# Start of sentence\n",
        "SOS = \"<start>\"\n",
        "# End of sentence\n",
        "EOS = \"<end>\"\n",
        "# Relevant punctuation\n",
        "PUNCTUATION = set(\"?,!.\")\n",
        "\n",
        "\n",
        "def load_dataset(filename: str) -> str:\n",
        "    \"\"\"\n",
        "    load dataset into memory\n",
        "    \"\"\"\n",
        "    with open(filename, mode=\"rt\", encoding=\"utf-8\") as fp:\n",
        "        return fp.read()\n",
        "\n",
        "\n",
        "def to_pairs(dataset: str, limit: int = None, shuffle=False) -> List[Tuple[str, str]]:\n",
        "    \"\"\"\n",
        "    Split dataset into pairs of sentences, discards dataset line info.\n",
        "\n",
        "    e.g.\n",
        "    input -> 'Go.\\tGeh.\\tCC-BY 2.0 (France) Attribution: tatoeba.org\n",
        "    #2877272 (CM) & #8597805 (Roujin)'\n",
        "    output -> [('Go.', 'Geh.')]\n",
        "\n",
        "    :param dataset: dataset containing examples of translations between\n",
        "    two languages\n",
        "    the examples are delimited by `\\n` and the contents of the lines are\n",
        "    delimited by `\\t`\n",
        "    :param limit: number that limit dataset size (optional)\n",
        "    :param shuffle: default is True\n",
        "    :return: list of pairs\n",
        "    \"\"\"\n",
        "    assert isinstance(limit, (int, type(None))), TypeError(\n",
        "        \"the limit value must be an integer\"\n",
        "    )\n",
        "    lines = dataset.strip().split(\"\\n\")\n",
        "    # Radom dataset\n",
        "    if shuffle is True:\n",
        "        random.shuffle(lines)\n",
        "    number_examples = limit or len(lines)  # if None get all\n",
        "    pairs = []\n",
        "    for line in lines[: abs(number_examples)]:\n",
        "        # take only source and target\n",
        "        src, trg, _ = line.split(\"\\t\")\n",
        "        pairs.append((src, trg))\n",
        "\n",
        "    # dataset size check\n",
        "    assert len(pairs) == number_examples\n",
        "    return pairs\n",
        "\n",
        "\n",
        "def separe_punctuation(token: str) -> str:\n",
        "    \"\"\"\n",
        "    Separe punctuation if exists\n",
        "    \"\"\"\n",
        "\n",
        "    if not set(token).intersection(PUNCTUATION):\n",
        "        return token\n",
        "    for p in PUNCTUATION:\n",
        "        token = f\" {p} \".join(token.split(p))\n",
        "    return \" \".join(token.split())\n",
        "\n",
        "\n",
        "def preprocess(sentence: str, add_start_end: bool=True) -> str:\n",
        "    \"\"\"\n",
        "    - convert lowercase\n",
        "    - remove numbers\n",
        "    - remove special characters\n",
        "    - separe punctuation\n",
        "    - add start-of-sentence <start> and end-of-sentence <end>\n",
        "\n",
        "    :param add_start_end: add SOS (start-of-sentence) and EOS (end-of-sentence)\n",
        "    \"\"\"\n",
        "    re_print = re.compile(f\"[^{re.escape(string.printable)}]\")\n",
        "    # convert lowercase and normalizing unicode characters\n",
        "    sentence = (\n",
        "        normalize(\"NFD\", sentence.lower()).encode(\"ascii\", \"ignore\").decode(\"UTF-8\")\n",
        "    )\n",
        "    cleaned_tokens = []\n",
        "    # tokenize sentence on white space\n",
        "    for token in sentence.split():\n",
        "        # removing non-printable chars form each token\n",
        "        token = re_print.sub(\"\", token).strip()\n",
        "        # ignore tokens with numbers\n",
        "        if re.findall(\"[0-9]\", token):\n",
        "            continue\n",
        "        # add space between words and punctuation eg: \"ok?go!\" => \"ok ? go !\"\n",
        "        token = separe_punctuation(token)\n",
        "        cleaned_tokens.append(token)\n",
        "\n",
        "    # rebuild sentence with space between tokens\n",
        "    sentence = \" \".join(cleaned_tokens)\n",
        "\n",
        "    # adding a start and an end token to the sentence\n",
        "    if add_start_end is True:\n",
        "        sentence = f\"{SOS} {sentence} {EOS}\"\n",
        "    return sentence\n",
        "\n",
        "\n",
        "def dataset_preprocess(dataset: List[Tuple[str, str]]) -> Tuple[List[str], List[str]]:\n",
        "    \"\"\"\n",
        "    Returns processed database\n",
        "\n",
        "    :param dataset: list of sentence pairs\n",
        "    :return: list of paralel data e.g. \n",
        "    (['first source sentence', 'second', ...], ['first target sentence', 'second', ...])\n",
        "    \"\"\"\n",
        "    source_cleaned = []\n",
        "    target_cleaned = []\n",
        "    for source, target in dataset:\n",
        "        source_cleaned.append(preprocess(source))\n",
        "        target_cleaned.append(preprocess(target))\n",
        "    return source_cleaned, target_cleaned\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5nDIELt9RH-w"
      },
      "source": [
        "## Create Dataset\n",
        "\n",
        "- limit number of examples\n",
        "- load dataset into pairs `[('Be nice.', 'Seien Sie nett!'), ('Beat it.', 'Geh weg!'), ...]`\n",
        "- preprocessing dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "colab_type": "code",
        "id": "GMxdlVU1X8yI",
        "outputId": "f4977f48-dbe9-4323-ec2a-a9b0cf8b1895"
      },
      "outputs": [],
      "source": [
        "NUM_EXAMPLES = 10000 # Limit dataset size\n",
        "\n",
        "# load from .txt\n",
        "filename = 'deu.txt' #change filename if necessary\n",
        "dataset = load_dataset(filename)\n",
        "# get pairs limited into 1000\n",
        "pairs = to_pairs(dataset, limit=NUM_EXAMPLES)\n",
        "print(f\"Dataset size: {len(pairs)}\")\n",
        "raw_data_en, raw_data_ge = dataset_preprocess(pairs)\n",
        "\n",
        "# show last 5 pairs\n",
        "for pair in zip(raw_data_en[-5:],raw_data_ge[-5:]):\n",
        "    print(pair)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Cfb66QxWYr6A"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "3oq60MBPSanQ"
      },
      "outputs": [],
      "source": [
        "en_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "en_tokenizer.fit_on_texts(raw_data_en)\n",
        "\n",
        "data_en = en_tokenizer.texts_to_sequences(raw_data_en)\n",
        "data_en = tf.keras.preprocessing.sequence.pad_sequences(data_en,padding='post')\n",
        "\n",
        "ge_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "ge_tokenizer.fit_on_texts(raw_data_ge)\n",
        "\n",
        "data_ge = ge_tokenizer.texts_to_sequences(raw_data_ge)\n",
        "data_ge = tf.keras.preprocessing.sequence.pad_sequences(data_ge,padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "XH5oSRNeSc1s"
      },
      "outputs": [],
      "source": [
        "def max_len(tensor):\n",
        "    #print( np.argmax([len(t) for t in tensor]))\n",
        "    return max( len(t) for t in tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KdM37lNBGXAj"
      },
      "source": [
        "## Model Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "EfiBUJM2Et6C"
      },
      "outputs": [],
      "source": [
        "X_train,  X_test, Y_train, Y_test = train_test_split(data_en,data_ge,test_size=0.2)\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = len(X_train)\n",
        "steps_per_epoch = BUFFER_SIZE//BATCH_SIZE\n",
        "embedding_dims = 256\n",
        "rnn_units = 1024\n",
        "dense_units = 1024\n",
        "Dtype = tf.float32   #used to initialize DecoderCell Zero state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ff_jQHLhGqJU"
      },
      "source": [
        "## Dataset Prepration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "colab_type": "code",
        "id": "b__1hPHVFALO",
        "outputId": "88d35286-184c-44e7-a16b-5559f22e2eb1"
      },
      "outputs": [],
      "source": [
        "Tx = max_len(data_en)\n",
        "Ty = max_len(data_ge)  \n",
        "\n",
        "input_vocab_size = len(en_tokenizer.word_index)+1  \n",
        "output_vocab_size = len(ge_tokenizer.word_index)+ 1\n",
        "dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "example_X, example_Y = next(iter(dataset))\n",
        "print(example_X.shape) \n",
        "print(example_Y.shape) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UQRgJcYgapqE"
      },
      "source": [
        "## Defining NMT Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "sGdakRtjaokF"
      },
      "outputs": [],
      "source": [
        "#ENCODER\n",
        "class EncoderNetwork(tf.keras.Model):\n",
        "    def __init__(self,input_vocab_size,embedding_dims, rnn_units ):\n",
        "        super().__init__()\n",
        "        self.encoder_embedding = tf.keras.layers.Embedding(input_dim=input_vocab_size,\n",
        "                                                           output_dim=embedding_dims)\n",
        "        self.encoder_rnnlayer = tf.keras.layers.LSTM(rnn_units,return_sequences=True, \n",
        "                                                     return_state=True )\n",
        "    \n",
        "#DECODER\n",
        "class DecoderNetwork(tf.keras.Model):\n",
        "    def __init__(self,output_vocab_size, embedding_dims, rnn_units):\n",
        "        super().__init__()\n",
        "        self.decoder_embedding = tf.keras.layers.Embedding(input_dim=output_vocab_size,\n",
        "                                                           output_dim=embedding_dims) \n",
        "        self.dense_layer = tf.keras.layers.Dense(output_vocab_size)\n",
        "        self.decoder_rnncell = tf.keras.layers.LSTMCell(rnn_units)\n",
        "        # Sampler\n",
        "        self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
        "        # Create attention mechanism with memory = None\n",
        "        self.attention_mechanism = self.build_attention_mechanism(dense_units,None,BATCH_SIZE*[Tx])\n",
        "        self.rnn_cell =  self.build_rnn_cell(BATCH_SIZE)\n",
        "        self.decoder = tfa.seq2seq.BasicDecoder(self.rnn_cell, sampler= self.sampler,\n",
        "                                                output_layer=self.dense_layer)\n",
        "\n",
        "    def build_attention_mechanism(self, units,memory, memory_sequence_length):\n",
        "        return tfa.seq2seq.LuongAttention(units, memory = memory, \n",
        "                                          memory_sequence_length=memory_sequence_length)\n",
        "        #return tfa.seq2seq.BahdanauAttention(units, memory = memory, memory_sequence_length=memory_sequence_length)\n",
        "\n",
        "    # wrap decodernn cell  \n",
        "    def build_rnn_cell(self, batch_size ):\n",
        "        rnn_cell = tfa.seq2seq.AttentionWrapper(self.decoder_rnncell, self.attention_mechanism,\n",
        "                                                attention_layer_size=dense_units)\n",
        "        return rnn_cell\n",
        "    \n",
        "    def build_decoder_initial_state(self, batch_size, encoder_state,Dtype):\n",
        "        decoder_initial_state = self.rnn_cell.get_initial_state(batch_size = batch_size, \n",
        "                                                                dtype = Dtype)\n",
        "        decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state) \n",
        "        return decoder_initial_state\n",
        "\n",
        "encoderNetwork = EncoderNetwork(input_vocab_size,embedding_dims, rnn_units)\n",
        "decoderNetwork = DecoderNetwork(output_vocab_size,embedding_dims, rnn_units)\n",
        "optimizer = tf.keras.optimizers.Adam()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NPwcfddTa0oB"
      },
      "source": [
        "## Initializing Training functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "x1BEqVyra2jW"
      },
      "outputs": [],
      "source": [
        "def loss_function(y_pred, y):\n",
        "   \n",
        "    #shape of y [batch_size, ty]\n",
        "    #shape of y_pred [batch_size, Ty, output_vocab_size] \n",
        "    sparsecategoricalcrossentropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
        "                                                                                  reduction='none')\n",
        "    loss = sparsecategoricalcrossentropy(y_true=y, y_pred=y_pred)\n",
        "    mask = tf.logical_not(tf.math.equal(y,0))   #output 0 for y=0 else output 1\n",
        "    mask = tf.cast(mask, dtype=loss.dtype)\n",
        "    loss = mask* loss\n",
        "    loss = tf.reduce_mean(loss)\n",
        "    return loss\n",
        "\n",
        "def train_step(input_batch, output_batch,encoder_initial_cell_state):\n",
        "    #initialize loss = 0\n",
        "    loss = 0\n",
        "    with tf.GradientTape() as tape:\n",
        "        encoder_emb_inp = encoderNetwork.encoder_embedding(input_batch)\n",
        "        a, a_tx, c_tx = encoderNetwork.encoder_rnnlayer(encoder_emb_inp, \n",
        "                                                        initial_state =encoder_initial_cell_state)\n",
        "\n",
        "        #[last step activations,last memory_state] of encoder passed as input to decoder Network\n",
        "        \n",
        "         \n",
        "        # Prepare correct Decoder input & output sequence data\n",
        "        decoder_input = output_batch[:,:-1] # ignore <end>\n",
        "        #compare logits with timestepped +1 version of decoder_input\n",
        "        decoder_output = output_batch[:,1:] #ignore <start>\n",
        "\n",
        "\n",
        "        # Decoder Embeddings\n",
        "        decoder_emb_inp = decoderNetwork.decoder_embedding(decoder_input)\n",
        "\n",
        "        #Setting up decoder memory from encoder output and Zero State for AttentionWrapperState\n",
        "        decoderNetwork.attention_mechanism.setup_memory(a)\n",
        "        decoder_initial_state = decoderNetwork.build_decoder_initial_state(BATCH_SIZE,\n",
        "                                                                           encoder_state=[a_tx, c_tx],\n",
        "                                                                           Dtype=tf.float32)\n",
        "        \n",
        "        #BasicDecoderOutput        \n",
        "        outputs, _, _ = decoderNetwork.decoder(decoder_emb_inp,initial_state=decoder_initial_state,\n",
        "                                               sequence_length=BATCH_SIZE*[Ty-1])\n",
        "\n",
        "        logits = outputs.rnn_output\n",
        "        #Calculate loss\n",
        "\n",
        "        loss = loss_function(logits, decoder_output)\n",
        "\n",
        "    #Returns the list of all layer variables / weights.\n",
        "    variables = encoderNetwork.trainable_variables + decoderNetwork.trainable_variables  \n",
        "    # differentiate loss wrt variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "\n",
        "    #grads_and_vars â€“ List of(gradient, variable) pairs.\n",
        "    grads_and_vars = zip(gradients,variables)\n",
        "    optimizer.apply_gradients(grads_and_vars)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "71Lkdx6GFb3A"
      },
      "outputs": [],
      "source": [
        "#RNN LSTM hidden and memory state initializer\n",
        "def initialize_initial_state():\n",
        "        return [tf.zeros((BATCH_SIZE, rnn_units)), tf.zeros((BATCH_SIZE, rnn_units))]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "v5uzLcu2bNX3"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "colab_type": "code",
        "id": "PvfD2SknWrt6",
        "outputId": "0a427bb7-8184-4076-97ca-f638116ca52b"
      },
      "outputs": [],
      "source": [
        "epochs = 15\n",
        "for i in range(1, epochs+1):\n",
        "\n",
        "    encoder_initial_cell_state = initialize_initial_state()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for ( batch , (input_batch, output_batch)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        batch_loss = train_step(input_batch, output_batch, encoder_initial_cell_state)\n",
        "        total_loss += batch_loss\n",
        "        if (batch+1)%5 == 0:\n",
        "            print(\"total loss: {} epoch {} batch {} \".format(batch_loss.numpy(), i, batch+1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nDyK-EGqbN5r"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "colab_type": "code",
        "id": "y98sfom7SuGy",
        "outputId": "00d94338-e841-4bd6-f9e3-509ef1f1a08b"
      },
      "outputs": [],
      "source": [
        "#In this section we evaluate our model on a raw_input converted to german, for this the entire sentence has to be passed\n",
        "#through the length of the model, for this we use greedsampler to run through the decoder\n",
        "#and the final embedding matrix trained on the data is used to generate embeddings\n",
        "input_raw='how are you'\n",
        "\n",
        "# We have a transcript file containing English-German pairs\n",
        "# Preprocess X\n",
        "input_raw = preprocess(input_raw, add_start_end=False)\n",
        "input_lines = [f'{SOS} {input_raw}']\n",
        "input_sequences = [[en_tokenizer.word_index[w] for w in line.split()] for line in input_lines]\n",
        "input_sequences = tf.keras.preprocessing.sequence.pad_sequences(input_sequences,\n",
        "                                                                maxlen=Tx, padding='post')\n",
        "inp = tf.convert_to_tensor(input_sequences)\n",
        "#print(inp.shape)\n",
        "inference_batch_size = input_sequences.shape[0]\n",
        "encoder_initial_cell_state = [tf.zeros((inference_batch_size, rnn_units)),\n",
        "                              tf.zeros((inference_batch_size, rnn_units))]\n",
        "encoder_emb_inp = encoderNetwork.encoder_embedding(inp)\n",
        "a, a_tx, c_tx = encoderNetwork.encoder_rnnlayer(encoder_emb_inp,\n",
        "                                                initial_state =encoder_initial_cell_state)\n",
        "print('a_tx :', a_tx.shape)\n",
        "print('c_tx :', c_tx.shape)\n",
        "\n",
        "start_tokens = tf.fill([inference_batch_size],ge_tokenizer.word_index[SOS])\n",
        "\n",
        "end_token = ge_tokenizer.word_index[EOS]\n",
        "\n",
        "greedy_sampler = tfa.seq2seq.GreedyEmbeddingSampler()\n",
        "\n",
        "decoder_input = tf.expand_dims([ge_tokenizer.word_index[SOS]]* inference_batch_size,1)\n",
        "decoder_emb_inp = decoderNetwork.decoder_embedding(decoder_input)\n",
        "\n",
        "decoder_instance = tfa.seq2seq.BasicDecoder(cell = decoderNetwork.rnn_cell, sampler = greedy_sampler,\n",
        "                                            output_layer=decoderNetwork.dense_layer)\n",
        "decoderNetwork.attention_mechanism.setup_memory(a)\n",
        "#pass [ last step activations , encoder memory_state ] as input to decoder for LSTM\n",
        "print(f\"decoder_initial_state = [a_tx, c_tx] : {np.array([a_tx, c_tx]).shape}\")\n",
        "decoder_initial_state = decoderNetwork.build_decoder_initial_state(inference_batch_size,\n",
        "                                                                   encoder_state=[a_tx, c_tx],\n",
        "                                                                   Dtype=tf.float32)\n",
        "print(f\"\"\"\n",
        "Compared to simple encoder-decoder without attention, the decoder_initial_state\n",
        "is an AttentionWrapperState object containing s_prev tensors and context and alignment vector\n",
        "\n",
        "decoder initial state shape: {np.array(decoder_initial_state).shape}\n",
        "decoder_initial_state tensor\n",
        "{decoder_initial_state}\n",
        "\"\"\")\n",
        "\n",
        "# Since we do not know the target sequence lengths in advance, we use maximum_iterations to limit the translation lengths.\n",
        "# One heuristic is to decode up to two times the source sentence lengths.\n",
        "maximum_iterations = tf.round(tf.reduce_max(Tx) * 2)\n",
        "\n",
        "#initialize inference decoder\n",
        "decoder_embedding_matrix = decoderNetwork.decoder_embedding.variables[0] \n",
        "(first_finished, first_inputs,first_state) = decoder_instance.initialize(decoder_embedding_matrix,\n",
        "                             start_tokens = start_tokens,\n",
        "                             end_token=end_token,\n",
        "                             initial_state = decoder_initial_state)\n",
        "#print( first_finished.shape)\n",
        "print(f\"first_inputs returns the same decoder_input i.e. embedding of  {SOS} : {first_inputs.shape}\")\n",
        "print(f\"start_index_emb_avg {tf.reduce_sum(tf.reduce_mean(first_inputs, axis=0))}\") # mean along the batch\n",
        "\n",
        "inputs = first_inputs\n",
        "state = first_state  \n",
        "predictions = np.empty((inference_batch_size,0), dtype = np.int32)                                                                             \n",
        "for j in range(maximum_iterations):\n",
        "    outputs, next_state, next_inputs, finished = decoder_instance.step(j,inputs,state)\n",
        "    inputs = next_inputs\n",
        "    state = next_state\n",
        "    outputs = np.expand_dims(outputs.sample_id,axis = -1)\n",
        "    predictions = np.append(predictions, outputs, axis = -1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iodjSItQds1t"
      },
      "source": [
        "## Final Translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "colab_type": "code",
        "id": "K6aWFB5IWlH2",
        "outputId": "2179c9a3-cb27-447a-ac94-0e5ab2920aff"
      },
      "outputs": [],
      "source": [
        "#prediction based on our sentence earlier\n",
        "print(\"English Sentence:\")\n",
        "print(input_raw)\n",
        "print(\"\\nGerman Translation:\")\n",
        "for i in range(len(predictions)):\n",
        "    line = predictions[i,:]\n",
        "    seq = list(itertools.takewhile( lambda index: index !=2, line))\n",
        "    print(\" \".join( [ge_tokenizer.index_word[w] for w in seq]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "g6Av-oPWvRc4"
      },
      "source": [
        "### The accuracy can be improved by implementing:\n",
        "* Beam Search or Lexicon Search\n",
        "* Bi-directional encoder-decoder model "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "networks_seq2seq_nmt.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
